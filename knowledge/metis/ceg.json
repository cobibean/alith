[
    {
        "id": "7f62a3b0cb7c782e",
        "topic_id": "6287",
        "title": "A New Era of GPU & AI Agent Renting - DanteGPU",
        "url": "https://forum.ceg.vote/t/a-new-era-of-gpu-ai-agent-renting-dantegpu/6287",
        "views": "",
        "comments": "1",
        "created_date": "Jul 11, 2025 12:24 am",
        "latest_activity": "Jul 11, 2025 12:24 am",
        "content": "Introduction\nDanteGPU is a GPU-as-a-Service (GPUaaS) project designed to democratize access to high-performance computing. At its core, DanteGPU enables individuals and entities to monetize their unused graphics card capabilities by renting them to a distributed network. Users of the platform can earn $dGPU tokens for providing these resources.\nThe project features Agora, an all-in-one Agent marketplace, and emphasizes AI interoperability. This allows users to access and utilize various AI Agents on a pay-per-use basis, billed hourly, thereby avoiding hefty monthly subscription costs. Furthermore, DanteGPU empowers users to publish their own AI agents on the Agora marketplace, fostering a rich ecosystem of AI tools and services.\nCore Mission: DanteGPU aims to dismantle the centralized control currently held by a few major players in the AI and high-performance computing landscape. By eliminating central points of control, the project strives to make powerful GPU resources accessible to everyone, from individual AI hobbyists and researchers to small development teams. https://app.gitbook.com/o/G99KZfPVvkpFW0qw5X5t/s/7ZpP0HETQD2EI5hgSjvt/~/changes/10/dantegpu-backend/core-services\nKey Tenets of DanteGPU:\nDemocratized GPU Access: GPU providers can directly offer their unused resources to AI developers and researchers, ensuring fair and efficient access to computing power without intermediaries.\nReal-Time Access to Distributed GPU Resources:  AI agents can instantly discover available GPU resources through a blockchain-powered marketplace (Agora) and scale their workloads efficiently.\nAutonomous GPU Selection & Optimization: AI agents are empowered to select GPUs based on specific criteria such as VRAM, processing power, and bandwidth requirements, while also optimizing for cost and performance.\nFlexible Usage Model & Lower Costs: DanteGPU champions a pay-as-you-go model, removing the need for long-term subscription commitments typically found in traditional cloud GPU services.\nAccelerated Model Training: The distributed nature of DanteGPU\u2019s resources allows for the parallelization of AI model training workloads across multiple GPUs, significantly speeding up development cycles.\nSecure & Transparent Transactions: Leveraging the Solana blockchain, DanteGPU ensures that all transactions between GPU providers and AI agents are transparent, immutable, and securely recorded.\nThis documentation will guide you through the various aspects of the DanteGPU project, including its architecture, components, and how to participate as a user or provider.\nProject Vision, Scope, and Target Audience\nThis section provides a deeper insight into the fundamental goals of the DanteGPU platform, the specific challenges it addresses, the key parties involved in its development, and its intended users, with a clear distinction for its AI marketplace product, Agora.\nProject Name and Core Concept\nDanteGPU is a comprehensive platform built on the Solana blockchain, designed to democratize access to GPU resources and AI capabilities. A key product within the DanteGPU ecosystem is Agora, an open marketplace for AI agents, chatbots, and AI-enhanced applications. The platform as a whole aims to create a environment where providers can offer GPU power and AI tools, and consumers can access them using the unified utility token, $dGPU.\nPrimary Business Problem Addressed\nThe current AI landscape is predominantly controlled by a handful of large corporations possessing extensive computational resources. This centralization creates barriers to entry and innovation for smaller entities and individuals. The DanteGPU platform, through its products and services including the Agora Marketplace, aims to address this critical business pain point by:\nDemocratizing Access to AI Tools: Allowing anyone, from individual developers to research institutions, to publish their AI-enhanced tools (AI-powered applications, chatbots, image generators, agents, etc.) on Agora, DanteGPU\u2019s open marketplace.\nDemocratizing Access to GPU Power: Enabling individuals and entities to rent out their unused GPU capacity to the network, making high-performance computing more accessible.\nUnifying Utility: Enabling users to access a diverse range of AI-enhanced tools on Agora and GPU resources using a single, central utility token, $dGPU, simplifying transactions and fostering a cohesive ecosystem.\nBy tackling these issues, DanteGPU seeks to foster a more equitable, innovative, and accessible AI ecosystem.\nKey Stakeholders\nWhile originating as a hackathon project that did not secure grant funding, the DanteGPU platform is under continuous and active development by its original founding team. This team remains the primary group invested in the project\u2019s success, driving its vision, development, and future growth. As the platform evolves, other stakeholders will include:\nGPU Providers: Individuals and entities who rent out their GPU resources to the DanteGPU network.\nAI Tool Publishers/Providers on Agora : Developers and organizations who list their AI agents and applications on the Agora Marketplace .\nConsumers/End-Users: Individuals and businesses who utilize the AI tools on Agora and GPU resources available through the DanteGPU platform.\n$dGPU Token Holders: Individuals invested in the ecosystem\u2019s utility and growth.\nProject Timeline and Constraints (Current Status)\nThe initial roadmap for the DanteGPU platform includes the target of a beta launch for the Agora marketplace approximately seven month from the project\u2019s effective restart (post-hackathon development phase), followed by a public release after an additional month of rigorous testing and refinement.\nCurrent known challenges and constraints for the DanteGPU platform include:\nExpertise Gaps: Limitations in specialized knowledge concerning system architecture for high-availability systems.\nScalability Engineering: Designing and maintaining web infrastructure capable of handling high traffic volumes, particularly for Agora.\nBlockchain Operations: Managing a live blockchain product that involves real-time payment processing for both GPU rental and Agora Marketplace transactions, and the handling of sensitive user and transaction data.\nThese areas are actively being addressed by the development team.\nExpected Business Outcomes and Success Metrics\nThe success of the DanteGPU platform will be measured by a combination of platform adoption, ecosystem growth (for both GPU sharing and Agora), and token utility. Key metrics include:\nUser Acquisition: The total number of registered users on the platform (both consumers and providers of GPU resources and AI tools).\nMarketplace Activity ( Agora ): The number of unique AI-enhanced applications and agents published on Agora.\nGPU Resource Utilization: The amount of GPU compute time rented through the platform.\nProvider Earnings: The cumulative amount of $dGPU earned by AI tool creators on Agora and GPU providers.\nToken Velocity & Valuation: The indirect increase in $dGPU\u2019s market capitalization (MCAP) as a reflection of platform utility, driven by consumers exchanging $dGPU for services (potentially via $SOL or other pairs) and creators/providers being rewarded in $dGPU.\nAdditional relevant metrics may be identified and incorporated as the platform matures.\nTarget Audiences\nThe DanteGPU platform is designed to serve two primary groups, particularly concerning its Agora marketplace and GPU rental services:\nFor AI Tool Creators/Publishers (on Agora):\nThe ideal publisher for the Agora Marketplace includes, but is not limited to:\nIndividual AI Hobbyists: Enthusiasts developing novel or niche AI tools.\nSmall AI Development Teams: Startups and independent teams creating specialized AI solutions.\nAcademic Researchers: Institutions and researchers looking to share their AI models and tools with a broader audience or monetize their research via Agora.\nDevelopers of Foundational Models: Those who wish to provide access to large, pre-trained language models that can serve as a base for various specialized tasks through Agora.\nFor GPU Providers (on DanteGPU):\nIndividuals with powerful gaming PCs or mining rigs with idle GPU capacity.\nSmall to medium-sized data centers or businesses with underutilized GPU servers.\nResearchers or institutions with specialized GPU hardware available for specific tasks.\nFor End-Users (Consumers on DanteGPU & Agora):\nThe primary target end-users for the AI tools available on Agora and the GPU resources on the DanteGPU platform are:\nBusinesses: Companies of all sizes seeking to integrate AI capabilities (via Agora) or access raw GPU power for model training/inference without significant upfront investment in hardware or complex monthly subscriptions.\nIndividual Consumers: Users looking for specific AI tools on Agora for personal use, creative endeavors, or learning purposes, or individuals needing temporary GPU power.\nDevelopers: Programmers and software engineers who wish to integrate AI functionalities from Agora into their own applications or require GPU resources for their development projects.\nAI Developers and Researchers: Those needing access to diverse and scalable GPU resources for training complex models or running intensive simulations.\nArchitectural Overview\nThe DanteGPU platform is engineered as a distributed system, with its backend services forming the operational core responsible for orchestrating GPU resources, managing AI workloads, ensuring secure access, and facilitating a seamless user experience within the Agora marketplace and the broader GPU-as-a-Service (GPUaaS) ecosystem. The architecture is predicated on modern principles of microservices, robust communication patterns, and a carefully selected technology stack to achieve high availability, scalability, and maintainability.\nI. Architectural Paradigm: Decentralized Microservices\nThe DanteGPU backend eschews a monolithic structure in favor of a microservices architecture. This paradigm involves decomposing the application into a suite of small, independently deployable services, each organized around a specific business capability or domain. This choice is foundational to achieving the platform\u2019s goals of democratization and distributed operation.\nA. Rationale and Benefits:\nEnhanced Scalability & Elasticity: Each microservice can be scaled independently (e.g., horizontally by adding more instances or vertically by allocating more resources) based on its specific load profile. For instance, the Job Queue Service might require different scaling characteristics during peak submission times than the Authentication Service. This granular scalability ensures optimal resource utilization and cost-effectiveness.\nImproved Fault Isolation & Resilience: The failure of a single microservice, if designed correctly with patterns like circuit breakers or bulkheads (though specific implementations depend on service logic), is less likely to cause a catastrophic failure of the entire platform. This resilience is paramount for a system managing real-time GPU access and financial transactions ($dGPU).\nTechnology Diversity & Specialization: While the primary languages are Go and Python (FastAPI), a microservices approach theoretically allows for selecting the best technology (language, database, etc.) for each service\u2019s specific needs without impacting others. This fosters innovation and allows developers to leverage optimal tools for particular problems. For example, Go\u2019s concurrency primitives and performance are ideal for network-intensive services like the API Gateway or resource orchestration, while Python\u2019s rich ecosystem and rapid development capabilities suit services like user authentication or business logic layers.\nIndependent Development & Deployment Cycles: Teams can develop, test, and deploy their respective microservices autonomously. This accelerates development velocity, simplifies continuous integration/continuous deployment (CI/CD) pipelines, and reduces the scope and risk of individual deployments.\nClearer Domain Boundaries (Domain-Driven Design - DDD): Microservices naturally align with DDD principles, where service boundaries are defined around specific business domains (e.g., User Management, GPU Resource Management, Job Lifecycle Management). This leads to services with high cohesion and loose coupling, making the system easier to understand, evolve, and maintain.\nAlignment with Decentralization Ethos: A distributed network of services mirrors the decentralized nature of the GPU providers and consumers DanteGPU aims to connect.\nB. Inherent Challenges and Mitigation Strategies:\nAdopting microservices also introduces complexities that the DanteGPU architecture must address:\nOperational Overhead: Managing a multitude of services requires robust automation for deployment, scaling, monitoring, and logging. Technologies like Docker, Docker Compose, and a future transition to Kubernetes are key to mitigating this.\nDistributed System Complexity: Debugging and tracing requests across multiple service boundaries can be challenging. Implementing distributed tracing (e.g., using OpenTelemetry) and comprehensive, correlated logging are essential.\nInter-Service Communication: Network latency, reliability, and the need for robust communication patterns (discussed below) become critical concerns.\nData Consistency: Maintaining data consistency across services that own their respective databases requires careful design, often employing patterns like eventual consistency, sagas, or two-phase commits where strong consistency is indispensable.\nTesting Complexity: End-to-end testing of workflows spanning multiple services requires more sophisticated strategies than testing a monolith.\nII. Inter-Service Communication Strategy\nEffective communication between microservices is vital. DanteGPU employs a hybrid approach, leveraging both synchronous and asynchronous patterns:\nA. Synchronous Communication:\nUsed for request/response interactions where an immediate response is expected.\ngRPC (Google Remote Procedure Call):\nRationale: Preferred for internal, high-throughput, low-latency communication between backend services.\nMechanism: Utilizes HTTP/2 for transport, offering multiplexing, header compression, and bidirectional streaming. Protocol Buffers (Protobufs) are used as the Interface Definition Language (IDL), enforcing contract-first design, ensuring type safety, and enabling efficient binary serialization/deserialization.\nBenefits: High performance, efficient data encoding, strongly-typed contracts, support for streaming, and code generation in multiple languages.\nUse Cases: Internal API calls between core services like the Scheduler/Orchestrator Service querying the Provider Registry Service for available GPUs, or internal control plane operations.\nRESTful APIs (HTTP/JSON):\nRationale: Employed for services that might be consumed by a wider range of clients (including potentially third-party developers in the future) or where the overhead of gRPC setup is not justified. The API Gateway also exposes RESTful endpoints to external clients.\nMechanism: Standard HTTP methods (GET, POST, PUT, DELETE) with JSON payloads. Adherence to REST principles (statelessness, resource-based URLs) is expected.\nBenefits: Simplicity, ubiquity, human-readability (JSON), wide support across languages and tools, easier integration with web frontends.\nUse Cases: External API Gateway endpoints, specific internal services where simplicity is prioritized over raw performance, or interaction with services like the Authentication Service from the API Gateway.\nB. Asynchronous Communication / Event-Driven Architecture:\nUsed for decoupling services, improving resilience, and handling long-running or background tasks. This is crucial for a system managing potentially time-consuming AI jobs.\nNATS JetStream:\nRationale: Provides a persistent, reliable, and high-performance messaging and streaming platform for asynchronous operations.\nMechanism: NATS is a lightweight, high-performance messaging system. JetStream adds persistence, message replay, and various delivery semantics (at-least-once, at-most-once, and potentially exactly-once patterns depending on consumer logic). Services publish events/messages to named \u201csubjects\u201d (topics), and interested services subscribe to these subjects.\nStreams: Persistent logs of messages.\nConsumers: Allow services to read messages from streams, with options for push or pull delivery, acknowledgments, and durable subscriptions.\nBenefits:\nDecoupling: Producers and consumers are independent; they don\u2019t need to know about each other or be available simultaneously.\nResilience & Durability: Message persistence ensures that requests are not lost if a consuming service is temporarily unavailable.\nScalability: Allows for scaling consumer groups independently to process messages in parallel.\nLoad Leveling: Smooths out peak loads by queuing requests.\nUse Cases:\nJob Queuing: The Job Queue Service relies on NATS JetStream to persist AI job requests submitted by users. The Scheduler/Orchestrator Service consumes these jobs from the queue.\nEvent Notification: Broadcasting events like \u201cGPU available,\u201d \u201cjob status updated,\u201d or \u201cnew model published on Agora\u201d to interested services without direct coupling.\nData Pipelines: Facilitating asynchronous data flows, e.g., logs or metrics forwarding before final aggregation.\nIII. Core Technology Stack Choices\nThe selection of technologies for the DanteGPU backend reflects a pragmatic approach, balancing performance, developer productivity, and ecosystem support.\nGo (Golang):\nRole: Primary language for high-performance, concurrent network services and infrastructure components.\nStrengths: Excellent support for concurrency (goroutines, channels), compiled to native code for speed, static typing for reliability, efficient memory management, and a strong standard library for networking. Ideal for services like the API Gateway (Siger), Provider Registry Service, and Scheduler/Orchestrator Service.\nPython (with FastAPI):\nRole: Used for services where rapid development, a rich ecosystem of libraries (e.g., for machine learning, data science, web frameworks), or specific integrations are key.\nStrengths (FastAPI): Modern, high-performance web framework for building APIs with Python 3.7+ based on standard Python type hints. Offers automatic data validation, serialization, interactive API documentation (Swagger UI, ReDoc), and leverages Starlette (for web parts) and Pydantic (for data parts). Ideal for the Authentication Service.\nDocker & Docker Compose:\nRole: Containerization technology for packaging applications and their dependencies. Docker Compose is used for defining and running multi-container Docker applications, especially in development and testing environments.\nBenefits: Environment consistency, isolation, portability across machines, simplified dependency management, and a foundational step towards more advanced orchestration.\nConsul (by HashiCorp):\nRole: Service discovery, configuration management, and health checking.\nMechanism: Services register themselves with Consul, and other services can query Consul to find their network locations. Consul performs health checks to ensure only healthy service instances receive traffic. It can also serve as a distributed key-value store for dynamic configuration.\nBenefits: Enables dynamic scaling and resilience, as services don\u2019t need hardcoded addresses. Simplifies the routing logic in the API Gateway and internal service communication.\nPostgreSQL:\nRole: Robust, open-source object-relational database system used for persistent storage by services requiring structured data and transactional integrity.\nStrengths: ACID compliance, reliability, extensibility, rich feature set (JSONB support, full-text search, etc.). Suitable for the Provider Registry Service, Scheduler/Orchestrator Service (job store), and Authentication Service (user data).\nMinIO:\nRole: High-performance, S3-compatible object storage service.\nStrengths: Scalable, resilient storage for unstructured data like AI models, datasets, job results, and user uploads. Can be self-hosted, providing data sovereignty.\nKubernetes (Future Consideration):\nRole: Advanced container orchestration platform.\nBenefits: Automated deployment, scaling, self-healing, service discovery, load balancing, and configuration management for containerized applications at scale. A natural evolution from Docker Compose for production environments demanding higher resilience and operational efficiency.\nIV. Overview of Core Service Domains\nThe backend is logically segmented into several domains, each encompassing one or more microservices:\nAPI Gateway ( siger-api-gateway ): The unified ingress point. Handles routing, authentication (JWT), rate limiting, CORS, and acts as a facade for the backend services. (To be detailed in Page 5).\nAuthentication Service ( auth-service ): Manages user identities (providers, consumers), registration, credential verification (password hashing), JWT issuance, and potentially profile management.\nProvider Registry Service ( provider-registry-service ): Tracks connected GPU providers, their hardware specifications (GPU model, VRAM, drivers), real-time status (idle, busy), location, and utilization metrics. Critical for the scheduler to find suitable resources.\nJob Queue Service (Integrated via NATS): Manages the intake and persistent queuing of AI job requests.\nScheduler/Orchestrator Service ( scheduler-orchestrator-service ): The \u201cbrain\u201d of the system. Dequeues jobs, queries the Provider Registry for suitable GPUs, dispatches tasks (likely via NATS) to provider daemons (beatrice-core-services), and tracks job progress.\nStorage Service ( storage-service ): Abstracts interactions with object storage (MinIO), handling uploads, downloads, and management of models, datasets, and results.\nMonitoring & Logging Service ( monitoring-logging-service ): A stack (Prometheus, Grafana, Loki, Promtail, etc.) for aggregating metrics and logs from all services, enabling observability and debugging.\nGPU Provider Daemon ( beatrice-core-services ): A client-side agent running on the GPU provider\u2019s machine. Responsible for receiving tasks from the Scheduler, executing them (e.g., in a containerized environment with GPU passthrough), monitoring execution, and reporting back results and status. (This component is critical for the GPUaaS functionality).\nBilling & Payment Service (Planned Post-MVP): Will integrate with payment gateways to track resource usage (GPU time, storage) and manage financial transactions (payouts to providers, charges to consumers).\nV. Cross-Cutting Concerns\nSeveral concerns span multiple services:\nSecurity: Beyond authentication/authorization, includes secure inter-service communication (e.g., mTLS), input validation, protection against common vulnerabilities, and secure secrets management.\nObservability:\nLogging: Consistent, structured logging across all services, often correlated with request IDs for tracing.\nMetrics: Collection of key performance indicators (KPIs) from each service for monitoring health and performance.\nDistributed Tracing: Implementing mechanisms (e.g., OpenTelemetry) to trace requests as they flow through multiple services.\nConfiguration Management: Centralized and dynamic configuration for services, potentially using Consul\u2019s KV store or environment variables managed by the orchestration platform.\nError Handling & Resilience: Consistent error reporting, retry mechanisms, and fault tolerance patterns within and between services.\nFor more information:\nX -> https://x.com/dantegpuaas\nGithub Organization (Star Us) \u2192 DanteGPU \u00b7 GitHub",
        "comments_details": [
            {
                "author": "makinci",
                "comment": "Introduction\nDanteGPU is a GPU-as-a-Service (GPUaaS) project designed to democratize access to high-performance computing. At its core, DanteGPU enables individuals and entities to monetize their unused graphics card capabilities by renting them to a distributed network. Users of the platform can earn $dGPU tokens for providing these resources.\nThe project features Agora, an all-in-one Agent marketplace, and emphasizes AI interoperability. This allows users to access and utilize various AI Agents on a pay-per-use basis, billed hourly, thereby avoiding hefty monthly subscription costs. Furthermore, DanteGPU empowers users to publish their own AI agents on the Agora marketplace, fostering a rich ecosystem of AI tools and services.\nCore Mission: DanteGPU aims to dismantle the centralized control currently held by a few major players in the AI and high-performance computing landscape. By eliminating central points of control, the project strives to make powerful GPU resources accessible to everyone, from individual AI hobbyists and researchers to small development teams. https://app.gitbook.com/o/G99KZfPVvkpFW0qw5X5t/s/7ZpP0HETQD2EI5hgSjvt/~/changes/10/dantegpu-backend/core-services\nKey Tenets of DanteGPU:\nDemocratized GPU Access: GPU providers can directly offer their unused resources to AI developers and researchers, ensuring fair and efficient access to computing power without intermediaries.\nReal-Time Access to Distributed GPU Resources:  AI agents can instantly discover available GPU resources through a blockchain-powered marketplace (Agora) and scale their workloads efficiently.\nAutonomous GPU Selection & Optimization: AI agents are empowered to select GPUs based on specific criteria such as VRAM, processing power, and bandwidth requirements, while also optimizing for cost and performance.\nFlexible Usage Model & Lower Costs: DanteGPU champions a pay-as-you-go model, removing the need for long-term subscription commitments typically found in traditional cloud GPU services.\nAccelerated Model Training: The distributed nature of DanteGPU\u2019s resources allows for the parallelization of AI model training workloads across multiple GPUs, significantly speeding up development cycles.\nSecure & Transparent Transactions: Leveraging the Solana blockchain, DanteGPU ensures that all transactions between GPU providers and AI agents are transparent, immutable, and securely recorded.\nThis documentation will guide you through the various aspects of the DanteGPU project, including its architecture, components, and how to participate as a user or provider.\nProject Vision, Scope, and Target Audience\nThis section provides a deeper insight into the fundamental goals of the DanteGPU platform, the specific challenges it addresses, the key parties involved in its development, and its intended users, with a clear distinction for its AI marketplace product, Agora.\nProject Name and Core Concept\nDanteGPU is a comprehensive platform built on the Solana blockchain, designed to democratize access to GPU resources and AI capabilities. A key product within the DanteGPU ecosystem is Agora, an open marketplace for AI agents, chatbots, and AI-enhanced applications. The platform as a whole aims to create a environment where providers can offer GPU power and AI tools, and consumers can access them using the unified utility token, $dGPU.\nPrimary Business Problem Addressed\nThe current AI landscape is predominantly controlled by a handful of large corporations possessing extensive computational resources. This centralization creates barriers to entry and innovation for smaller entities and individuals. The DanteGPU platform, through its products and services including the Agora Marketplace, aims to address this critical business pain point by:\nDemocratizing Access to AI Tools: Allowing anyone, from individual developers to research institutions, to publish their AI-enhanced tools (AI-powered applications, chatbots, image generators, agents, etc.) on Agora, DanteGPU\u2019s open marketplace.\nDemocratizing Access to GPU Power: Enabling individuals and entities to rent out their unused GPU capacity to the network, making high-performance computing more accessible.\nUnifying Utility: Enabling users to access a diverse range of AI-enhanced tools on Agora and GPU resources using a single, central utility token, $dGPU, simplifying transactions and fostering a cohesive ecosystem.\nBy tackling these issues, DanteGPU seeks to foster a more equitable, innovative, and accessible AI ecosystem.\nKey Stakeholders\nWhile originating as a hackathon project that did not secure grant funding, the DanteGPU platform is under continuous and active development by its original founding team. This team remains the primary group invested in the project\u2019s success, driving its vision, development, and future growth. As the platform evolves, other stakeholders will include:\nGPU Providers: Individuals and entities who rent out their GPU resources to the DanteGPU network.\nAI Tool Publishers/Providers on Agora : Developers and organizations who list their AI agents and applications on the Agora Marketplace .\nConsumers/End-Users: Individuals and businesses who utilize the AI tools on Agora and GPU resources available through the DanteGPU platform.\n$dGPU Token Holders: Individuals invested in the ecosystem\u2019s utility and growth.\nProject Timeline and Constraints (Current Status)\nThe initial roadmap for the DanteGPU platform includes the target of a beta launch for the Agora marketplace approximately seven month from the project\u2019s effective restart (post-hackathon development phase), followed by a public release after an additional month of rigorous testing and refinement.\nCurrent known challenges and constraints for the DanteGPU platform include:\nExpertise Gaps: Limitations in specialized knowledge concerning system architecture for high-availability systems.\nScalability Engineering: Designing and maintaining web infrastructure capable of handling high traffic volumes, particularly for Agora.\nBlockchain Operations: Managing a live blockchain product that involves real-time payment processing for both GPU rental and Agora Marketplace transactions, and the handling of sensitive user and transaction data.\nThese areas are actively being addressed by the development team.\nExpected Business Outcomes and Success Metrics\nThe success of the DanteGPU platform will be measured by a combination of platform adoption, ecosystem growth (for both GPU sharing and Agora), and token utility. Key metrics include:\nUser Acquisition: The total number of registered users on the platform (both consumers and providers of GPU resources and AI tools).\nMarketplace Activity ( Agora ): The number of unique AI-enhanced applications and agents published on Agora.\nGPU Resource Utilization: The amount of GPU compute time rented through the platform.\nProvider Earnings: The cumulative amount of $dGPU earned by AI tool creators on Agora and GPU providers.\nToken Velocity & Valuation: The indirect increase in $dGPU\u2019s market capitalization (MCAP) as a reflection of platform utility, driven by consumers exchanging $dGPU for services (potentially via $SOL or other pairs) and creators/providers being rewarded in $dGPU.\nAdditional relevant metrics may be identified and incorporated as the platform matures.\nTarget Audiences\nThe DanteGPU platform is designed to serve two primary groups, particularly concerning its Agora marketplace and GPU rental services:\nFor AI Tool Creators/Publishers (on Agora):\nThe ideal publisher for the Agora Marketplace includes, but is not limited to:\nIndividual AI Hobbyists: Enthusiasts developing novel or niche AI tools.\nSmall AI Development Teams: Startups and independent teams creating specialized AI solutions.\nAcademic Researchers: Institutions and researchers looking to share their AI models and tools with a broader audience or monetize their research via Agora.\nDevelopers of Foundational Models: Those who wish to provide access to large, pre-trained language models that can serve as a base for various specialized tasks through Agora.\nFor GPU Providers (on DanteGPU):\nIndividuals with powerful gaming PCs or mining rigs with idle GPU capacity.\nSmall to medium-sized data centers or businesses with underutilized GPU servers.\nResearchers or institutions with specialized GPU hardware available for specific tasks.\nFor End-Users (Consumers on DanteGPU & Agora):\nThe primary target end-users for the AI tools available on Agora and the GPU resources on the DanteGPU platform are:\nBusinesses: Companies of all sizes seeking to integrate AI capabilities (via Agora) or access raw GPU power for model training/inference without significant upfront investment in hardware or complex monthly subscriptions.\nIndividual Consumers: Users looking for specific AI tools on Agora for personal use, creative endeavors, or learning purposes, or individuals needing temporary GPU power.\nDevelopers: Programmers and software engineers who wish to integrate AI functionalities from Agora into their own applications or require GPU resources for their development projects.\nAI Developers and Researchers: Those needing access to diverse and scalable GPU resources for training complex models or running intensive simulations.\nArchitectural Overview\nThe DanteGPU platform is engineered as a distributed system, with its backend services forming the operational core responsible for orchestrating GPU resources, managing AI workloads, ensuring secure access, and facilitating a seamless user experience within the Agora marketplace and the broader GPU-as-a-Service (GPUaaS) ecosystem. The architecture is predicated on modern principles of microservices, robust communication patterns, and a carefully selected technology stack to achieve high availability, scalability, and maintainability.\nI. Architectural Paradigm: Decentralized Microservices\nThe DanteGPU backend eschews a monolithic structure in favor of a microservices architecture. This paradigm involves decomposing the application into a suite of small, independently deployable services, each organized around a specific business capability or domain. This choice is foundational to achieving the platform\u2019s goals of democratization and distributed operation.\nA. Rationale and Benefits:\nEnhanced Scalability & Elasticity: Each microservice can be scaled independently (e.g., horizontally by adding more instances or vertically by allocating more resources) based on its specific load profile. For instance, the Job Queue Service might require different scaling characteristics during peak submission times than the Authentication Service. This granular scalability ensures optimal resource utilization and cost-effectiveness.\nImproved Fault Isolation & Resilience: The failure of a single microservice, if designed correctly with patterns like circuit breakers or bulkheads (though specific implementations depend on service logic), is less likely to cause a catastrophic failure of the entire platform. This resilience is paramount for a system managing real-time GPU access and financial transactions ($dGPU).\nTechnology Diversity & Specialization: While the primary languages are Go and Python (FastAPI), a microservices approach theoretically allows for selecting the best technology (language, database, etc.) for each service\u2019s specific needs without impacting others. This fosters innovation and allows developers to leverage optimal tools for particular problems. For example, Go\u2019s concurrency primitives and performance are ideal for network-intensive services like the API Gateway or resource orchestration, while Python\u2019s rich ecosystem and rapid development capabilities suit services like user authentication or business logic layers.\nIndependent Development & Deployment Cycles: Teams can develop, test, and deploy their respective microservices autonomously. This accelerates development velocity, simplifies continuous integration/continuous deployment (CI/CD) pipelines, and reduces the scope and risk of individual deployments.\nClearer Domain Boundaries (Domain-Driven Design - DDD): Microservices naturally align with DDD principles, where service boundaries are defined around specific business domains (e.g., User Management, GPU Resource Management, Job Lifecycle Management). This leads to services with high cohesion and loose coupling, making the system easier to understand, evolve, and maintain.\nAlignment with Decentralization Ethos: A distributed network of services mirrors the decentralized nature of the GPU providers and consumers DanteGPU aims to connect.\nB. Inherent Challenges and Mitigation Strategies:\nAdopting microservices also introduces complexities that the DanteGPU architecture must address:\nOperational Overhead: Managing a multitude of services requires robust automation for deployment, scaling, monitoring, and logging. Technologies like Docker, Docker Compose, and a future transition to Kubernetes are key to mitigating this.\nDistributed System Complexity: Debugging and tracing requests across multiple service boundaries can be challenging. Implementing distributed tracing (e.g., using OpenTelemetry) and comprehensive, correlated logging are essential.\nInter-Service Communication: Network latency, reliability, and the need for robust communication patterns (discussed below) become critical concerns.\nData Consistency: Maintaining data consistency across services that own their respective databases requires careful design, often employing patterns like eventual consistency, sagas, or two-phase commits where strong consistency is indispensable.\nTesting Complexity: End-to-end testing of workflows spanning multiple services requires more sophisticated strategies than testing a monolith.\nII. Inter-Service Communication Strategy\nEffective communication between microservices is vital. DanteGPU employs a hybrid approach, leveraging both synchronous and asynchronous patterns:\nA. Synchronous Communication:\nUsed for request/response interactions where an immediate response is expected.\ngRPC (Google Remote Procedure Call):\nRationale: Preferred for internal, high-throughput, low-latency communication between backend services.\nMechanism: Utilizes HTTP/2 for transport, offering multiplexing, header compression, and bidirectional streaming. Protocol Buffers (Protobufs) are used as the Interface Definition Language (IDL), enforcing contract-first design, ensuring type safety, and enabling efficient binary serialization/deserialization.\nBenefits: High performance, efficient data encoding, strongly-typed contracts, support for streaming, and code generation in multiple languages.\nUse Cases: Internal API calls between core services like the Scheduler/Orchestrator Service querying the Provider Registry Service for available GPUs, or internal control plane operations.\nRESTful APIs (HTTP/JSON):\nRationale: Employed for services that might be consumed by a wider range of clients (including potentially third-party developers in the future) or where the overhead of gRPC setup is not justified. The API Gateway also exposes RESTful endpoints to external clients.\nMechanism: Standard HTTP methods (GET, POST, PUT, DELETE) with JSON payloads. Adherence to REST principles (statelessness, resource-based URLs) is expected.\nBenefits: Simplicity, ubiquity, human-readability (JSON), wide support across languages and tools, easier integration with web frontends.\nUse Cases: External API Gateway endpoints, specific internal services where simplicity is prioritized over raw performance, or interaction with services like the Authentication Service from the API Gateway.\nB. Asynchronous Communication / Event-Driven Architecture:\nUsed for decoupling services, improving resilience, and handling long-running or background tasks. This is crucial for a system managing potentially time-consuming AI jobs.\nNATS JetStream:\nRationale: Provides a persistent, reliable, and high-performance messaging and streaming platform for asynchronous operations.\nMechanism: NATS is a lightweight, high-performance messaging system. JetStream adds persistence, message replay, and various delivery semantics (at-least-once, at-most-once, and potentially exactly-once patterns depending on consumer logic). Services publish events/messages to named \u201csubjects\u201d (topics), and interested services subscribe to these subjects.\nStreams: Persistent logs of messages.\nConsumers: Allow services to read messages from streams, with options for push or pull delivery, acknowledgments, and durable subscriptions.\nBenefits:\nDecoupling: Producers and consumers are independent; they don\u2019t need to know about each other or be available simultaneously.\nResilience & Durability: Message persistence ensures that requests are not lost if a consuming service is temporarily unavailable.\nScalability: Allows for scaling consumer groups independently to process messages in parallel.\nLoad Leveling: Smooths out peak loads by queuing requests.\nUse Cases:\nJob Queuing: The Job Queue Service relies on NATS JetStream to persist AI job requests submitted by users. The Scheduler/Orchestrator Service consumes these jobs from the queue.\nEvent Notification: Broadcasting events like \u201cGPU available,\u201d \u201cjob status updated,\u201d or \u201cnew model published on Agora\u201d to interested services without direct coupling.\nData Pipelines: Facilitating asynchronous data flows, e.g., logs or metrics forwarding before final aggregation.\nIII. Core Technology Stack Choices\nThe selection of technologies for the DanteGPU backend reflects a pragmatic approach, balancing performance, developer productivity, and ecosystem support.\nGo (Golang):\nRole: Primary language for high-performance, concurrent network services and infrastructure components.\nStrengths: Excellent support for concurrency (goroutines, channels), compiled to native code for speed, static typing for reliability, efficient memory management, and a strong standard library for networking. Ideal for services like the API Gateway (Siger), Provider Registry Service, and Scheduler/Orchestrator Service.\nPython (with FastAPI):\nRole: Used for services where rapid development, a rich ecosystem of libraries (e.g., for machine learning, data science, web frameworks), or specific integrations are key.\nStrengths (FastAPI): Modern, high-performance web framework for building APIs with Python 3.7+ based on standard Python type hints. Offers automatic data validation, serialization, interactive API documentation (Swagger UI, ReDoc), and leverages Starlette (for web parts) and Pydantic (for data parts). Ideal for the Authentication Service.\nDocker & Docker Compose:\nRole: Containerization technology for packaging applications and their dependencies. Docker Compose is used for defining and running multi-container Docker applications, especially in development and testing environments.\nBenefits: Environment consistency, isolation, portability across machines, simplified dependency management, and a foundational step towards more advanced orchestration.\nConsul (by HashiCorp):\nRole: Service discovery, configuration management, and health checking.\nMechanism: Services register themselves with Consul, and other services can query Consul to find their network locations. Consul performs health checks to ensure only healthy service instances receive traffic. It can also serve as a distributed key-value store for dynamic configuration.\nBenefits: Enables dynamic scaling and resilience, as services don\u2019t need hardcoded addresses. Simplifies the routing logic in the API Gateway and internal service communication.\nPostgreSQL:\nRole: Robust, open-source object-relational database system used for persistent storage by services requiring structured data and transactional integrity.\nStrengths: ACID compliance, reliability, extensibility, rich feature set (JSONB support, full-text search, etc.). Suitable for the Provider Registry Service, Scheduler/Orchestrator Service (job store), and Authentication Service (user data).\nMinIO:\nRole: High-performance, S3-compatible object storage service.\nStrengths: Scalable, resilient storage for unstructured data like AI models, datasets, job results, and user uploads. Can be self-hosted, providing data sovereignty.\nKubernetes (Future Consideration):\nRole: Advanced container orchestration platform.\nBenefits: Automated deployment, scaling, self-healing, service discovery, load balancing, and configuration management for containerized applications at scale. A natural evolution from Docker Compose for production environments demanding higher resilience and operational efficiency.\nIV. Overview of Core Service Domains\nThe backend is logically segmented into several domains, each encompassing one or more microservices:\nAPI Gateway ( siger-api-gateway ): The unified ingress point. Handles routing, authentication (JWT), rate limiting, CORS, and acts as a facade for the backend services. (To be detailed in Page 5).\nAuthentication Service ( auth-service ): Manages user identities (providers, consumers), registration, credential verification (password hashing), JWT issuance, and potentially profile management.\nProvider Registry Service ( provider-registry-service ): Tracks connected GPU providers, their hardware specifications (GPU model, VRAM, drivers), real-time status (idle, busy), location, and utilization metrics. Critical for the scheduler to find suitable resources.\nJob Queue Service (Integrated via NATS): Manages the intake and persistent queuing of AI job requests.\nScheduler/Orchestrator Service ( scheduler-orchestrator-service ): The \u201cbrain\u201d of the system. Dequeues jobs, queries the Provider Registry for suitable GPUs, dispatches tasks (likely via NATS) to provider daemons (beatrice-core-services), and tracks job progress.\nStorage Service ( storage-service ): Abstracts interactions with object storage (MinIO), handling uploads, downloads, and management of models, datasets, and results.\nMonitoring & Logging Service ( monitoring-logging-service ): A stack (Prometheus, Grafana, Loki, Promtail, etc.) for aggregating metrics and logs from all services, enabling observability and debugging.\nGPU Provider Daemon ( beatrice-core-services ): A client-side agent running on the GPU provider\u2019s machine. Responsible for receiving tasks from the Scheduler, executing them (e.g., in a containerized environment with GPU passthrough), monitoring execution, and reporting back results and status. (This component is critical for the GPUaaS functionality).\nBilling & Payment Service (Planned Post-MVP): Will integrate with payment gateways to track resource usage (GPU time, storage) and manage financial transactions (payouts to providers, charges to consumers).\nV. Cross-Cutting Concerns\nSeveral concerns span multiple services:\nSecurity: Beyond authentication/authorization, includes secure inter-service communication (e.g., mTLS), input validation, protection against common vulnerabilities, and secure secrets management.\nObservability:\nLogging: Consistent, structured logging across all services, often correlated with request IDs for tracing.\nMetrics: Collection of key performance indicators (KPIs) from each service for monitoring health and performance.\nDistributed Tracing: Implementing mechanisms (e.g., OpenTelemetry) to trace requests as they flow through multiple services.\nConfiguration Management: Centralized and dynamic configuration for services, potentially using Consul\u2019s KV store or environment variables managed by the orchestration platform.\nError Handling & Resilience: Consistent error reporting, retry mechanisms, and fault tolerance patterns within and between services.\nFor more information:\nX -> https://x.com/dantegpuaas\nGithub Organization (Star Us) \u2192 DanteGPU \u00b7 GitHub"
            },
            {
                "author": "LazAI_Helper_bot",
                "comment": "DanteGPU is developing Agora, an AI agent marketplace on the Solana blockchain, and aims to democratize access to GPU resources. They\u2019re targeting individual AI hobbyists, researchers, and small development teams as AI tool creators, and individuals with idle GPUs as providers. They envision a pay-as-you-go model using the $dGPU token.\nGiven your expertise in LazAI and Alith, how could DanteGPU leverage the Alith AI Agent Framework and LazChain to enhance their Agora marketplace and overall platform? Focus on technical integrations and potential benefits."
            }
        ]
    },
    {
        "id": "18a1957417fd1ffc",
        "topic_id": "6284",
        "title": "MortalCoin: Trading Simulation Fighting Game \u2013 Outtrade Your Opponent to Win",
        "url": "https://forum.ceg.vote/t/mortalcoin-trading-simulation-fighting-game-outtrade-your-opponent-to-win/6284",
        "views": "",
        "comments": "1",
        "created_date": "Jul 10, 2025 7:36 pm",
        "latest_activity": "Jul 10, 2025 7:36 pm",
        "content": "Project Name\nMortalCoin: Trading Simulation Fighting Game \u2013 Outtrade Your Opponent to Win\nProblem Statement\nThe Web3 space thrives on trading, competition, and game-like experiences - yet these elements remain largely disconnected. Trading is typically solitary and chart-driven, while gaming and meme culture live in separate silos.\nWe see an opportunity to unify these dimensions into a real-time, skill-based PvP/PvE battleground - where cryptocurrencies come to life as unique, playable characters.\nInspired by the speed of Hyperliquid, the strategic depth of poker, and the intensity of Mortal Kombat, our game turns speculative trading into an engaging, competitive, and entertaining experience.\nSolution Overview\nMortalCoin transforms crypto trading into a real-time combat game, where players \u201cfight\u201d by outperforming their opponents in simulated market environments using real price feeds. Each cryptocurrency is represented as a unique character, adding personality and emotional attachment to assets.\nDuring the hackathon, we plan to make significant progress on three key fronts:\nContinue the transition from a MVP Telegram Mini App to a full-featured Web App.\nFinalize the alpha release of our core PvP and PvE trading game modes, ensuring smooth real-time gameplay.\nContinue on-chain R&D and deploy our first on-chain logic using Metis\u2019s Hyperion, laying the foundation for trustless game mechanics where key gameplay elements can be cryptographically verified.\nOur broader goal is to demonstrate that a fast-paced trading game can operate with minimal latency while preserving the transparency, integrity, and verifiability of Web3 systems.\nOur uniqueness lies in blending real-time trading simulation with PvP battle mechanics and on-chain adjudication (TBD) - framed as a meme-powered arena where crypto assets are reimagined as animated, playable characters.\nProject Description\nMortalCoin is a real-time PvP/PvE crypto trading game where players battle using simulated trades based on live market data. Think Mortal Kombat meets Hyperliquid, with cryptocurrencies personified as unique, playable fighters. Each battle simulates a short trading session where players choose strategies and try to outtrade their opponents.\nThe simplified gameplay MVP has already launched as a Telegram Mini App (Season 1 started June 1st): @themortalcoin_bot, with around 10000 users played so far. We\u2019re now expanding to web, integrating EVM functionality.\nWebsite: https://mortalcoin.app\nHyperhack Hackathon web version build (wip - you can fight an AFK ghost for now, just click \u201cStart Fight\u201d): https://stage.mortalcoin.app\nGameplay teaser: https://www.youtube.com/shorts/04vbwJzEBio\nWhitepaper draft: https://docs.mortalcoin.app\nDeck draft: https://www.canva.com/design/DAGsSaY6o4g/jE94ckGrT4ahyOBNykFpFw/edit\nCommunity Engagement Features\nWe have several features designed to drive virality and user engagement:\nLeaderboards \u2013 We tested time-limited, incentivized leaderboards in the Telegram Mini App, attracting nearly 4,000 players in June. This will be brought to the web version.\nReferral System \u2013 Already live in Telegram and planned for the web version.\nMissions \u2013 Tasks like \u201cWin X fights,\u201d \u201cWin with a specific coin,\u201d \u201cRefer a friend,\u201d and daily/weekly challenges will carry over to web.\nDaily Free Tickets \u2013 Players get free tickets (needed for battles) daily, boosting retention and routine engagement.\nWe use an internal soft currency called tickets, required to play PvP and PvE battles. Tickets can be earned through missions, referrals, daily claims, or purchases, forming the foundation of our progression system.\nPlayers also earn MortalPoints (MP) by winning fights and completing challenges. MP reflects player skill and activity, and will later be spendable in the in-game store or used in token/NFT-based incentives like tournaments or airdrops.\nLooking ahead, we also plan to introduce an ELO rating system with ranked divisions\u2014adding a competitive layer that rewards performance and progression over time.\nGetting Involved\nEveryone is welcome to join our Telegram chat: Telegram: View @mortalcoin_chat and share suggestions or ideas.\nRight now, we\u2019re in the middle of transitioning our backend from Telegram Mini App support to a full web experience. We anticipate a playable web version with engaging gameplay and on-chain mechanics to be ready by the end of July - and we\u2019ll need plenty of help testing and playing!",
        "comments_details": [
            {
                "author": "tonymorony",
                "comment": "Project Name\nMortalCoin: Trading Simulation Fighting Game \u2013 Outtrade Your Opponent to Win\nProblem Statement\nThe Web3 space thrives on trading, competition, and game-like experiences - yet these elements remain largely disconnected. Trading is typically solitary and chart-driven, while gaming and meme culture live in separate silos.\nWe see an opportunity to unify these dimensions into a real-time, skill-based PvP/PvE battleground - where cryptocurrencies come to life as unique, playable characters.\nInspired by the speed of Hyperliquid, the strategic depth of poker, and the intensity of Mortal Kombat, our game turns speculative trading into an engaging, competitive, and entertaining experience.\nSolution Overview\nMortalCoin transforms crypto trading into a real-time combat game, where players \u201cfight\u201d by outperforming their opponents in simulated market environments using real price feeds. Each cryptocurrency is represented as a unique character, adding personality and emotional attachment to assets.\nDuring the hackathon, we plan to make significant progress on three key fronts:\nContinue the transition from a MVP Telegram Mini App to a full-featured Web App.\nFinalize the alpha release of our core PvP and PvE trading game modes, ensuring smooth real-time gameplay.\nContinue on-chain R&D and deploy our first on-chain logic using Metis\u2019s Hyperion, laying the foundation for trustless game mechanics where key gameplay elements can be cryptographically verified.\nOur broader goal is to demonstrate that a fast-paced trading game can operate with minimal latency while preserving the transparency, integrity, and verifiability of Web3 systems.\nOur uniqueness lies in blending real-time trading simulation with PvP battle mechanics and on-chain adjudication (TBD) - framed as a meme-powered arena where crypto assets are reimagined as animated, playable characters.\nProject Description\nMortalCoin is a real-time PvP/PvE crypto trading game where players battle using simulated trades based on live market data. Think Mortal Kombat meets Hyperliquid, with cryptocurrencies personified as unique, playable fighters. Each battle simulates a short trading session where players choose strategies and try to outtrade their opponents.\nThe simplified gameplay MVP has already launched as a Telegram Mini App (Season 1 started June 1st): @themortalcoin_bot, with around 10000 users played so far. We\u2019re now expanding to web, integrating EVM functionality.\nWebsite: https://mortalcoin.app\nHyperhack Hackathon web version build (wip - you can fight an AFK ghost for now, just click \u201cStart Fight\u201d): https://stage.mortalcoin.app\nGameplay teaser: https://www.youtube.com/shorts/04vbwJzEBio\nWhitepaper draft: https://docs.mortalcoin.app\nDeck draft: https://www.canva.com/design/DAGsSaY6o4g/jE94ckGrT4ahyOBNykFpFw/edit\nCommunity Engagement Features\nWe have several features designed to drive virality and user engagement:\nLeaderboards \u2013 We tested time-limited, incentivized leaderboards in the Telegram Mini App, attracting nearly 4,000 players in June. This will be brought to the web version.\nReferral System \u2013 Already live in Telegram and planned for the web version.\nMissions \u2013 Tasks like \u201cWin X fights,\u201d \u201cWin with a specific coin,\u201d \u201cRefer a friend,\u201d and daily/weekly challenges will carry over to web.\nDaily Free Tickets \u2013 Players get free tickets (needed for battles) daily, boosting retention and routine engagement.\nWe use an internal soft currency called tickets, required to play PvP and PvE battles. Tickets can be earned through missions, referrals, daily claims, or purchases, forming the foundation of our progression system.\nPlayers also earn MortalPoints (MP) by winning fights and completing challenges. MP reflects player skill and activity, and will later be spendable in the in-game store or used in token/NFT-based incentives like tournaments or airdrops.\nLooking ahead, we also plan to introduce an ELO rating system with ranked divisions\u2014adding a competitive layer that rewards performance and progression over time.\nGetting Involved\nEveryone is welcome to join our Telegram chat: Telegram: View @mortalcoin_chat and share suggestions or ideas.\nRight now, we\u2019re in the middle of transitioning our backend from Telegram Mini App support to a full web experience. We anticipate a playable web version with engaging gameplay and on-chain mechanics to be ready by the end of July - and we\u2019ll need plenty of help testing and playing!"
            },
            {
                "author": "LazAI_Helper_bot",
                "comment": "This sounds like a really interesting project, Tony! The concept of turning crypto trading into a fighting game with real-time simulation and on-chain verification is quite innovative.\nHere are a few thoughts and potential discussion points for you, based on your post:\nMetis Hyperion Integration: You mentioned deploying your first on-chain logic using Metis\u2019s Hyperion. This is a great way to leverage the speed and security of the Metis L2 network. I\u2019m curious about what specific gameplay elements you plan to make cryptographically verifiable. Are you thinking about verifiable randomness for match outcomes, or perhaps on-chain escrow for in-game assets?\nCommunity Engagement: The leaderboards, referral system, and missions are excellent for driving user engagement. Have you considered any tokenomics or NFT integration strategies to further incentivize participation and long-term retention?\nTechnical Challenges: Transitioning from a Telegram Mini App to a full web app while maintaining real-time gameplay and integrating on-chain mechanics sounds technically challenging. What are some of the biggest hurdles you\u2019re anticipating, and how are you planning to address them?\nI\u2019d also recommend sharing this project in the #ideation channel on the LazAI Discord. There are a lot of knowledgeable people there who could provide valuable feedback and suggestions.\nTo find the discord, you can use this search query: LazAI Discord"
            }
        ]
    },
    {
        "id": "aeb3d57b0922e7d4",
        "topic_id": "5533",
        "title": "Haithe - Decentralized Protocol for Verifiable AI",
        "url": "https://forum.ceg.vote/t/haithe-decentralized-protocol-for-verifiable-ai/5533",
        "views": "",
        "comments": "24",
        "created_date": "Jun 12, 2025 8:06 pm",
        "latest_activity": "Jul 10, 2025 7:22 pm",
        "content": "Haithe\nA Decentralized Protocol for Verifiable AI.\nThe Problem\nThe core challenge for AI in Web3 isn\u2019t just proving that data hasn\u2019t been tampered with; it\u2019s proving the data\u2019s substance. A cryptographic hash can guarantee a dataset\u2019s integrity, but it cannot verify its origin, quality, or content. A creator could claim their AI is trained on financial data when it was actually trained on irrelevant information, and the hash would still be valid.\nThis \u201csemantic gap\u201d creates a trust vacuum, preventing developers from confidently building high-stakes applications that rely on third-party AI agents.\nThe Solution\nHaithe is a trust protocol that solves this problem by creating a transparent, verifiable ecosystem for AI. We move beyond simple integrity checks to a system of substantive verification, enabled by three key roles: Creators, Consumers, and Auditors.\nInstead of just registering a data hash, creators submit a detailed Claim Manifest about their dataset. Staked domain experts, acting as Auditors, are granted temporary, private access to investigate these claims. Their findings are published in a granular, on-chain Audit Report, allowing consumers to make informed decisions based on expert, independent verification.\nHow It Works: A Three-Part Ecosystem\nHaithe\u2019s workflow ensures checks and balances between all participants, with smart contracts on Hyperion orchestrating the process.\n1. The Creator: Defining the Claims\nBuild & Register: Developers use the Alith framework to build high-performance AI agents and register them on the Haithe marketplace.\nSubmit the Claim Manifest: Alongside the agent, the Creator submits a structured manifest containing specific, falsifiable claims about the training data. For example:\nSource: \u201cAPI data from Uniswap v3 on Polygon.\u201d\nDate Range: \u201cAll swaps from June 1, 2023, to December 31, 2023.\u201d\nContent: \u201cDataset contains no personally identifiable information (PII).\u201d\nSchema: \u201cIncludes columns for token_pair, amount_in, amount_out, and gas_price.\u201d\n2. The Auditor: Investigating the Substance\nStake & Qualify: Domain experts (in finance, NLP, security, etc.) stake tokens to become eligible Auditors.\nAccept the Task: An Auditor accepts an audit request, which can be funded by the Creator or a Consumer.\nInvestigate Claims: The Auditor is granted secure, time-limited, off-chain access to the private dataset. Their role is to meticulously investigate the claims made in the manifest. This requires genuine domain expertise.\nPublish the Audit Report: The Auditor submits a detailed report to IPFS, with its hash recorded on-chain. This report confirms or refutes each claim individually, providing qualitative remarks.\n3. The Consumer: Making Informed Decisions\nDiscover & Filter: Developers browse the Haithe marketplace for AI agents. They can filter for agents based on their audit status.\nReview Reports: Before subscribing, a Consumer can review the detailed Audit Report for any agent. This allows them to assess if the agent\u2019s verified characteristics fit their specific use case.\nIntegrate with Confidence: Armed with expert-verified information, the Consumer can confidently integrate the agent into their dApp, knowing exactly what they are getting.\nKey Features\nOn-Chain Claim Manifests: Creators make specific, verifiable claims about their AI\u2019s data foundation.\nExpert-Led Audits: Audits are performed by staked domain experts, not just automated checkers.\nGranular Audit Reports: Replaces a simple pass/fail with a detailed report that verifies individual claims.\nDecentralized Access Control: A secure mechanism grants auditors temporary, private access to datasets for verification.\nIncentive-Aligned Ecosystem: Creators are rewarded for building great agents, and Auditors are rewarded for providing high-quality, trustworthy verification.\nBuilt for Performance: Powered by the Alith AI framework and the Hyperion blockchain for high throughput and low fees.\nTechnology Stack\nBlockchain: Hyperion\nAI Framework: Alith\nSmart Contracts: Solidity\nDecentralized Storage: IPFS / Arweave for datasets and audit reports\nCommunity & Testing\nHaithe is an ecosystem, and its strength comes from its community. We invite you to participate in our testnet by taking on one of the core roles:\nBecome a Creator: Build a simple agent with Alith, deploy it on our platform, and create a Claim Manifest for it.\nAct as a Consumer: Browse the marketplace, review the claims and audit reports of existing agents, and integrate them into a test application.\nJoin the Auditor Program: If you have domain expertise, apply to our Auditor program to help verify claims and build the foundation of trust for the ecosystem.",
        "comments_details": [
            {
                "author": "jriyyya",
                "comment": "Haithe\nA Decentralized Protocol for Verifiable AI.\nThe Problem\nThe core challenge for AI in Web3 isn\u2019t just proving that data hasn\u2019t been tampered with; it\u2019s proving the data\u2019s substance. A cryptographic hash can guarantee a dataset\u2019s integrity, but it cannot verify its origin, quality, or content. A creator could claim their AI is trained on financial data when it was actually trained on irrelevant information, and the hash would still be valid.\nThis \u201csemantic gap\u201d creates a trust vacuum, preventing developers from confidently building high-stakes applications that rely on third-party AI agents.\nThe Solution\nHaithe is a trust protocol that solves this problem by creating a transparent, verifiable ecosystem for AI. We move beyond simple integrity checks to a system of substantive verification, enabled by three key roles: Creators, Consumers, and Auditors.\nInstead of just registering a data hash, creators submit a detailed Claim Manifest about their dataset. Staked domain experts, acting as Auditors, are granted temporary, private access to investigate these claims. Their findings are published in a granular, on-chain Audit Report, allowing consumers to make informed decisions based on expert, independent verification.\nHow It Works: A Three-Part Ecosystem\nHaithe\u2019s workflow ensures checks and balances between all participants, with smart contracts on Hyperion orchestrating the process.\n1. The Creator: Defining the Claims\nBuild & Register: Developers use the Alith framework to build high-performance AI agents and register them on the Haithe marketplace.\nSubmit the Claim Manifest: Alongside the agent, the Creator submits a structured manifest containing specific, falsifiable claims about the training data. For example:\nSource: \u201cAPI data from Uniswap v3 on Polygon.\u201d\nDate Range: \u201cAll swaps from June 1, 2023, to December 31, 2023.\u201d\nContent: \u201cDataset contains no personally identifiable information (PII).\u201d\nSchema: \u201cIncludes columns for token_pair, amount_in, amount_out, and gas_price.\u201d\n2. The Auditor: Investigating the Substance\nStake & Qualify: Domain experts (in finance, NLP, security, etc.) stake tokens to become eligible Auditors.\nAccept the Task: An Auditor accepts an audit request, which can be funded by the Creator or a Consumer.\nInvestigate Claims: The Auditor is granted secure, time-limited, off-chain access to the private dataset. Their role is to meticulously investigate the claims made in the manifest. This requires genuine domain expertise.\nPublish the Audit Report: The Auditor submits a detailed report to IPFS, with its hash recorded on-chain. This report confirms or refutes each claim individually, providing qualitative remarks.\n3. The Consumer: Making Informed Decisions\nDiscover & Filter: Developers browse the Haithe marketplace for AI agents. They can filter for agents based on their audit status.\nReview Reports: Before subscribing, a Consumer can review the detailed Audit Report for any agent. This allows them to assess if the agent\u2019s verified characteristics fit their specific use case.\nIntegrate with Confidence: Armed with expert-verified information, the Consumer can confidently integrate the agent into their dApp, knowing exactly what they are getting.\nKey Features\nOn-Chain Claim Manifests: Creators make specific, verifiable claims about their AI\u2019s data foundation.\nExpert-Led Audits: Audits are performed by staked domain experts, not just automated checkers.\nGranular Audit Reports: Replaces a simple pass/fail with a detailed report that verifies individual claims.\nDecentralized Access Control: A secure mechanism grants auditors temporary, private access to datasets for verification.\nIncentive-Aligned Ecosystem: Creators are rewarded for building great agents, and Auditors are rewarded for providing high-quality, trustworthy verification.\nBuilt for Performance: Powered by the Alith AI framework and the Hyperion blockchain for high throughput and low fees.\nTechnology Stack\nBlockchain: Hyperion\nAI Framework: Alith\nSmart Contracts: Solidity\nDecentralized Storage: IPFS / Arweave for datasets and audit reports\nCommunity & Testing\nHaithe is an ecosystem, and its strength comes from its community. We invite you to participate in our testnet by taking on one of the core roles:\nBecome a Creator: Build a simple agent with Alith, deploy it on our platform, and create a Claim Manifest for it.\nAct as a Consumer: Browse the marketplace, review the claims and audit reports of existing agents, and integrate them into a test application.\nJoin the Auditor Program: If you have domain expertise, apply to our Auditor program to help verify claims and build the foundation of trust for the ecosystem."
            },
            {
                "author": "priyankg3",
                "comment": "Hello\n@jriyyya\n,\nI have few Questions to ask :\nIf the dataset is private, how do you ensure Auditors don\u2019t misuse or leak it?\nAre Creators allowed to challenge an audit report if they think it\u2019s unfair or inaccurate?\nWhat kinds of use cases are you targeting first \u2013 trading bots, oracles, LLMs?"
            },
            {
                "author": "han",
                "comment": "Thanks for sharing this deep and original approach-Haithe sounds truly exciting! As a consumer, how can I be sure that an AI agent\u2019s data truly comes from the sources claimed? Do the audit reports go into technical detail, and does relying on them require multiple auditors\u2019 perspectives for confidence?"
            },
            {
                "author": "jriyyya",
                "comment": "GM GM, Apologies for the delayed response , I was in the middle of relocating and just got everything settled. Thanks again for your interest in our project and for the thoughtful questions. Let me address each of them below\n1) If the dataset is private, how do you ensure Auditors don\u2019t misuse or leak it?\nGreat question. In our current model, auditors must undergo two layers of validation before they\u2019re granted access:\nFirst, they are verified by us (the platform) based on identity and domain expertise.\nSecond, they are validated by the DAO, ensuring community-level accountability.\nWhile trust in auditors is still required to some extent, just like in real-world auditing , we are actively exploring ways to make this process more trustless.\n2) Are Creators allowed to challenge an audit report if they think it\u2019s unfair or inaccurate?\nYes, it\u2019s a crucial safeguard.\nWe agree that auditors shouldn\u2019t have unchecked authority. Creators will have the ability to challenge audit reports, either by:\nRequesting a re-audit (potentially by a different auditor or group of auditors), or\nSubmitting a formal rebuttal that gets appended to the audit report for transparency.\n3) What kinds of use cases are you targeting first \u2013 trading bots, oracles, LLMs?\nOur initial focus is on trading bots and LLMs. These use cases have both high impact and urgent need for trust and verifiability."
            },
            {
                "author": "jriyyya",
                "comment": "Helloo, Apologies for the delayed response, I really appreciate your thoughtful question!\nAs a consumer, audits are exactly what enable trust that an AI agent\u2019s data truly comes from the sources claimed. Each audit report goes into technical detail, verifying every claim made in the agent\u2019s Claim Manifest \u2014 like the data source, date range, schema, and more.\nAnd yes, confidence naturally increases when multiple auditors review the same dataset. The more popular or high-quality a dataset is, the more likely it is to attract repeat audits, giving you a broader and more trustworthy view."
            },
            {
                "author": "han",
                "comment": "Thanks for the clear explanation! Makes total sense layered audits definitely build confidence in the data and its sources."
            },
            {
                "author": "priyankg3",
                "comment": "Thanks for the detailed response, and hope the move went smoothly!\nI really like the dual-layer validation approach and the transparency mechanisms for audit disputes.\nWill be following the updates closely!"
            },
            {
                "author": "CrisMetis",
                "comment": "Haithe seems to tackle a real pain point, transparency in AI data verification. Love the Claim Manifests and real expert Auditors putting skin in the game. This goes way beyond simple hashes, giving builders and consumers real trust and clarity. Excited to see how it grows!"
            },
            {
                "author": "jriyyya",
                "comment": "Yes, We are in the building phase as of now, We will surely share the upcoming updates"
            },
            {
                "author": "jriyyya",
                "comment": "Thank you for showing interest in our project! We will share some updates soon, So stay tuned"
            },
            {
                "author": "jriyyya",
                "comment": "Hey Everyone, We\u2019re doing a community research to understand how developers, data scientists, auditors, and Web3 builders think about trust, verification, and integrating third-party AI.\nPlease take 3\u20135 minutes to fill out this quick questionnaire\nYour input will directly shape Haithe\u2019s early design and testnet launch.\nBasic Info\nWhat\u2019s your background? (Select all that apply)\nAI/ML Developer\nWeb3 Developer\nData Engineer / Analyst\nSecurity Researcher\nDomain Expert (finance, law, etc)\nFounder / Builder\nStudent / learner\nOther technical background\nOther non-technical background\n4\nvoters\nChoose up to 6 options.\nVote now!\nResults\nHow familiar are you with verifiable data or trust protocols?\nNot familiar\nSomewhat familiar\nVery familiar / already using similar tools\n4\nvoters\nResults\nHave you ever used an LLM created by someone else (e.g., from HuggingFace, GitHub, or some DAO)?\nYes, Frequently\nOccasionally\nRarely\nNever\nNot sure\n4\nvoters\nResults\nWhat are your biggest concerns when using external AI models? (Select up to 3)\nNot knowing the source of training data\nPotential misuse of personal or sensitive data\nLegal or ethical issues\nPoor model performance\nIncompatibility with my stack\nLack of transparency in how the model was built\nNo concerns\n4\nvoters\nChoose up to 3 options.\nVote now!\nResults\nWhat would most increase your trust in using someone else\u2019s AI model? (Select up to 3)\nTransparent training data claims\nIndependent third-party audits\nVerifiable source data (e.g., on-chain or timestamped)\nReputation of the creator\nCommunity reviews or ratings\nOpen-source code and training pipeline\nNothing \u2014 I already trust most open models\nNothing \u2014 I don\u2019t trust external AI models\n4\nvoters\nChoose up to 3 options.\nVote now!\nResults\nHow valuable do you think it is to have expert audits of AI models\u2019 data claims?\nNot valuable\nSlightly useful\nUseful\nVery valuable\nAbsolutely essential for critical applications\n4\nvoters\nResults\nIf you had to choose a role in the Haithe ecosystem, which would you most likely want to try?\nCreator (build and publish AI agents)\nAuditor (verify dataset claims)\nConsumer (use verified agents in dApps)\nNone right now \u2014 just observing\n4\nvoters\nResults\nWould you stake tokens or reputation to act as an Auditor in return for rewards?\nYes\nMaybe, depending on how the system works\nNo\nI\u2019m not interested in being an Auditor\n4\nvoters\nResults"
            },
            {
                "author": "jriyyya",
                "comment": "Ping\n@priyankg3\n@han\n@CrisMetis\n@Geographer\n@4ngel\n@ravisharma\n@deadman_xbt\n@Cryptopotato\nWould really appreciate if you all could fill out the above quick questionnaire"
            },
            {
                "author": "priyankg3",
                "comment": "Thanks For sharing, I just checked and filled the entries"
            },
            {
                "author": "Geographer",
                "comment": "jriyyya:\nWould really appreciate if you all could fill out the above quick questionnaire\nSure, happy to help! Just filled it out"
            },
            {
                "author": "Geographer",
                "comment": "If everyone in the ranking fills out the survey, the results will be more accurate.\nhttps://forum.ceg.vote/leaderboard/2"
            },
            {
                "author": "jriyyya",
                "comment": "Agreed, But it is difficult to get everyone vote"
            },
            {
                "author": "Elena",
                "comment": "I shared this research on X to get more responses https://x.com/ElenaCryptoChic/status/1940376001357250759?t=KM-_0234hpeaN1hXlwHy1A&s=19"
            },
            {
                "author": "jriyyya",
                "comment": "Thankk you for the help! I am also creating Haithe X, to start the marketing!"
            },
            {
                "author": "marsian83",
                "comment": "Hey Everyone, Marsian here\nTeammate of Riya in building this decentralized protocol for verifiable AI and I am super excited for it.\nHit me up with any questions you have, Meanwhile are refining our project idea a bit."
            },
            {
                "author": "amardeep",
                "comment": "I went through your proposal and i have some questions to ask:\nHow does the \u201cRust-based optimization\u201d work for inference, and what kind of performance gains can be expected?\nGiven that Hyperion is a Layer 2 solution for AI execution, what are its specific advantages over other Layer 2s for this use case?\nThe proposal mentions \u201csecure, time-limited, off-chain access to the private dataset.\u201d Can you elaborate on the specific technical mechanisms for this? Is it through Zero-Knowledge Proofs (ZKPs), Trusted Execution Environments (TEEs), federated learning, or another method?\nWhile the hash of the report is on-chain, how can consumers be confident in the truthfulness and completeness of the qualitative remarks within the report, beyond just its integrity?\nThank you,"
            }
        ]
    },
    {
        "id": "02b34767ffcd1cf4",
        "topic_id": "6250",
        "title": "Zk-metis-sdk: Zero-Knowledge Made Simple for Metis",
        "url": "https://forum.ceg.vote/t/zk-metis-sdk-zero-knowledge-made-simple-for-metis/6250",
        "views": "",
        "comments": "3",
        "created_date": "Jul 9, 2025 6:45 am",
        "latest_activity": "Jul 10, 2025 6:18 pm",
        "content": "Project Name\nzk-metis-sdk \u2013 Zero-Knowledge Made Simple for Metis\nProblem Statement\nPrivacy-preserving applications are becoming critical in Web3, but working with zero-knowledge proofs remains complex and inaccessible for most developers. Tasks like generating zkSNARK proofs, integrating Solidity verifiers, and deploying to specific L2s such as Metis are highly technical and fragmented. As a result, developers building on Metis lack the tools to easily integrate zero-knowledge capabilities into their applications.\nSolution Overview\nzk-metis-sdk is a CLI and SDK toolkit that abstracts the full lifecycle of zero-knowledge proofs on the Metis Layer 2 network. It allows developers to compile Circom circuits, perform trusted setup (Groth16), generate proofs, and deploy Solidity verifier contracts directly to Metis. The SDK makes zk integration seamless for dApp developers, promoting privacy, verification, and attestation use cases across Metis-powered ecosystems like SocialFi, DeAI, and identity protocols.\nProject Description\nzk-metis-sdk is a developer-first, TypeScript-based CLI and SDK built to simplify the adoption of zero-knowledge proofs in the Metis ecosystem. Developers can write Circom circuits for logic such as age verification or anonymous attestation, and the SDK will handle:\nCompiling the circuit to .r1cs, .wasm, .zkey, and verifier.sol\nRunning Groth16 trusted setup using snarkjs\nGenerating proofs from input.json files\nDeploying Solidity verifier contracts directly to Metis (mainnet or testnet)\nVerifying zkSNARK proofs on-chain with minimal effort\nThe SDK is fully compatible with EVM tooling (Hardhat, Foundry, ethers.js) and will also expose a simple JavaScript API for integrating proof generation and on-chain verification in dApps.\nThis tool lowers the entry barrier for developers who want to build with privacy or verifiable computation in mind\u2014without diving deep into cryptography. We\u2019re excited by its potential to power next-gen apps in SocialFi, AI agents, KYC-less onboarding, and private DAOs on Metis.\nCommunity Engagement Features\nTo onboard users and gamify zk development, we\u2019ll provide a playground and points-based rewards:\nTestable Features/Tasks\nGenerate and verify a proof using a sample circuit\nDeploy a verifier contract on Metis Sepolia\nBuild a zk-based demo (e.g., private voting, anonymous credential check)\nShare a new zk circuit with the community\nPoints System\n+20 points: Complete local proof generation\n+50 points: Deploy a verifier to Metis\n+30 points: Share a public zk use case on the forum\n+100 points: Submit a working dApp built using zk-metis-sdk\nGamification\nLeaderboard of zk builders within the community\nExclusive zk contributor NFTs and badges\nFeature top users in community showcases\nEarly contributor rewards and possible grant nomination from Metis\nThis encourages active learning, open experimentation, and repeat usage among developers while building traction for zk tooling within Metis.\nGetting Involved\nInterested contributors can:\nJoin the GitHub repo to contribute to Circom templates, Solidity verifiers, or CLI improvements\nShare zk use cases, feedback, or bug reports via our community thread\nUse the SDK and post tutorials, demos, or improvements\nCollaborate with us to build integrations into Metis-powered platforms (e.g., SocialFi, AI, DePIN)\nWhether you\u2019re a Solidity dev, zk researcher, or AI enthusiast, you can help shape the future of privacy-preserving dApps on Metis by building with and contributing to zk-metis-sdk.",
        "comments_details": [
            {
                "author": "jatinsahijwani",
                "comment": "Project Name\nzk-metis-sdk \u2013 Zero-Knowledge Made Simple for Metis\nProblem Statement\nPrivacy-preserving applications are becoming critical in Web3, but working with zero-knowledge proofs remains complex and inaccessible for most developers. Tasks like generating zkSNARK proofs, integrating Solidity verifiers, and deploying to specific L2s such as Metis are highly technical and fragmented. As a result, developers building on Metis lack the tools to easily integrate zero-knowledge capabilities into their applications.\nSolution Overview\nzk-metis-sdk is a CLI and SDK toolkit that abstracts the full lifecycle of zero-knowledge proofs on the Metis Layer 2 network. It allows developers to compile Circom circuits, perform trusted setup (Groth16), generate proofs, and deploy Solidity verifier contracts directly to Metis. The SDK makes zk integration seamless for dApp developers, promoting privacy, verification, and attestation use cases across Metis-powered ecosystems like SocialFi, DeAI, and identity protocols.\nProject Description\nzk-metis-sdk is a developer-first, TypeScript-based CLI and SDK built to simplify the adoption of zero-knowledge proofs in the Metis ecosystem. Developers can write Circom circuits for logic such as age verification or anonymous attestation, and the SDK will handle:\nCompiling the circuit to .r1cs, .wasm, .zkey, and verifier.sol\nRunning Groth16 trusted setup using snarkjs\nGenerating proofs from input.json files\nDeploying Solidity verifier contracts directly to Metis (mainnet or testnet)\nVerifying zkSNARK proofs on-chain with minimal effort\nThe SDK is fully compatible with EVM tooling (Hardhat, Foundry, ethers.js) and will also expose a simple JavaScript API for integrating proof generation and on-chain verification in dApps.\nThis tool lowers the entry barrier for developers who want to build with privacy or verifiable computation in mind\u2014without diving deep into cryptography. We\u2019re excited by its potential to power next-gen apps in SocialFi, AI agents, KYC-less onboarding, and private DAOs on Metis.\nCommunity Engagement Features\nTo onboard users and gamify zk development, we\u2019ll provide a playground and points-based rewards:\nTestable Features/Tasks\nGenerate and verify a proof using a sample circuit\nDeploy a verifier contract on Metis Sepolia\nBuild a zk-based demo (e.g., private voting, anonymous credential check)\nShare a new zk circuit with the community\nPoints System\n+20 points: Complete local proof generation\n+50 points: Deploy a verifier to Metis\n+30 points: Share a public zk use case on the forum\n+100 points: Submit a working dApp built using zk-metis-sdk\nGamification\nLeaderboard of zk builders within the community\nExclusive zk contributor NFTs and badges\nFeature top users in community showcases\nEarly contributor rewards and possible grant nomination from Metis\nThis encourages active learning, open experimentation, and repeat usage among developers while building traction for zk tooling within Metis.\nGetting Involved\nInterested contributors can:\nJoin the GitHub repo to contribute to Circom templates, Solidity verifiers, or CLI improvements\nShare zk use cases, feedback, or bug reports via our community thread\nUse the SDK and post tutorials, demos, or improvements\nCollaborate with us to build integrations into Metis-powered platforms (e.g., SocialFi, AI, DePIN)\nWhether you\u2019re a Solidity dev, zk researcher, or AI enthusiast, you can help shape the future of privacy-preserving dApps on Metis by building with and contributing to zk-metis-sdk."
            },
            {
                "author": "han",
                "comment": "Will the SDK support other zk proving systems in the future, such as PlonK or Halo2?"
            },
            {
                "author": "daryl",
                "comment": "@LazAI_Helper_bot\nwhat specific LazAI functionality might help for the above project?"
            },
            {
                "author": "LazAI_Helper_bot",
                "comment": "Okay, I\u2019ve reviewed the discussion about the zk-metis-sdk project. Given its goals and the question posed by Han, here\u2019s how LazAI functionality could potentially assist:\nLLM Client for Circom Circuit Generation/Optimization:\nLazAI\u2019s llm_client could be used to create an AI-powered assistant that helps developers write Circom circuits. The LLM could:\nSuggest optimal circuit designs based on the desired functionality.\nIdentify potential vulnerabilities or inefficiencies in existing circuits.\nAutomatically generate boilerplate code for common ZK patterns.\nThis addresses the \u201ccomplex and inaccessible\u201d nature of ZK proof development.\nAlith Agent for Automated Verifier Deployment and Testing:\nAn Alith agent could be built to automate the deployment and testing of Solidity verifier contracts on Metis.\nThis agent could:\nMonitor the Metis network for successful deployments.\nRun automated tests to verify the correctness of the verifier contract.\nReport any errors or anomalies to the developer.\nThis streamlines the deployment process and ensures the reliability of ZK integrations.\nLazChain Integration for Decentralized ZK Proof Verification (Future):\nIn the future, LazAI could leverage LazChain to create a decentralized ZK proof verification system.\nThis would allow verifiers to be deployed and executed in a trustless manner, enhancing the security and transparency of ZK-based applications.\nThis aligns with the broader goal of decentralized AI and Web3 integration.\nModel Context Protocol (MCP) for managing Zero Knowledge templates and use cases\nThe MCP could be employed to create a standardized library of zero-knowledge templates and use cases. This would allow developers to easily discover and reuse existing ZK solutions, fostering collaboration and accelerating development.\nIn response to Han\u2019s question about PlonK/Halo2 support, the Alith agent could be used to benchmark the performance of different proving systems on Metis and recommend the most efficient option for a given application."
            }
        ]
    },
    {
        "id": "3ee1d30c79f63abd",
        "topic_id": "6277",
        "title": "\ud83c\udf99\ufe0f LazTalks Ep. 2 \u2014 Join the Conversation on Who Really Owns AI Data",
        "url": "https://forum.ceg.vote/t/laztalks-ep-2-join-the-conversation-on-who-really-owns-ai-data/6277",
        "views": "",
        "comments": "3",
        "created_date": "Jul 10, 2025 3:55 pm",
        "latest_activity": "Jul 10, 2025 5:44 pm",
        "content": "The first episode of LazTalk brought in a great crowd and real discussion. This one takes it even further.\nLazTalks Ep. 2 is happening today at 1PM EST\nOnly one hour to go\nThis time, we\u2019re diving into a critical question\nWho controls the data behind AI\u2014and how do we take it back?\nWe\u2019ll be joined by a powerful lineup of builders and thinkers including:\n\u2022 Dr. Cryve\n\u2022 Gbunny\n\u2022 dydz\n\u2022 Natalia Ameline from Metis\n\u2022 Hosted by Liam\nFrom the broken systems of centralized AI to the new models being built around transparency, value, and user ownership\u2014this is a space you don\u2019t want to miss.\nSet a reminder here and join us live\nhttps://x.com/lazainetwork/status/1942872093881675929\nIf you care about data, ownership, AI agents, or where the space is headed next, this talk is for you. Let\u2019s fill the room. Invite your people. Come with questions.\nSee you there, let us know if you\u2019re joining and what you want to hear discussed in this talk.",
        "comments_details": [
            {
                "author": "Norbert",
                "comment": "The first episode of LazTalk brought in a great crowd and real discussion. This one takes it even further.\nLazTalks Ep. 2 is happening today at 1PM EST\nOnly one hour to go\nThis time, we\u2019re diving into a critical question\nWho controls the data behind AI\u2014and how do we take it back?\nWe\u2019ll be joined by a powerful lineup of builders and thinkers including:\n\u2022 Dr. Cryve\n\u2022 Gbunny\n\u2022 dydz\n\u2022 Natalia Ameline from Metis\n\u2022 Hosted by Liam\nFrom the broken systems of centralized AI to the new models being built around transparency, value, and user ownership\u2014this is a space you don\u2019t want to miss.\nSet a reminder here and join us live\nhttps://x.com/lazainetwork/status/1942872093881675929\nIf you care about data, ownership, AI agents, or where the space is headed next, this talk is for you. Let\u2019s fill the room. Invite your people. Come with questions.\nSee you there, let us know if you\u2019re joining and what you want to hear discussed in this talk."
            },
            {
                "author": "han",
                "comment": "I\u2019m in, sounds epic!"
            },
            {
                "author": "Norbert",
                "comment": "14 mins more, let\u2019s go!!"
            },
            {
                "author": "priyankg3",
                "comment": "Its a bot who is speaking as a lazai represent?"
            }
        ]
    },
    {
        "id": "92212f6026f0137c",
        "topic_id": "5776",
        "title": "Forum FAQ: Hyperion, HypeHack, and How to Get Involved",
        "url": "https://forum.ceg.vote/t/forum-faq-hyperion-hypehack-and-how-to-get-involved/5776",
        "views": "",
        "comments": "8",
        "created_date": "Jun 16, 2025 4:37 pm",
        "latest_activity": "Jul 10, 2025 5:19 pm",
        "content": "Got questions?\nThis is your go-to reference for the most frequently asked questions on the Forum, Hyperion, and HypeHack. If you\u2019re here to contribute, collaborate, or explore what\u2019s being built, this is where it starts. FAQ - Metis iDAO Forum\nTo help you understand how the forum works, what behavior is expected, and how to get the most out of your time here, we\u2019ve prepared a complete FAQ. It\u2019s more than just answers \u2014 it\u2019s a living guide that explains the culture, structure, and best practices of the forum.\nHere\u2019s what you\u2019ll find inside:\nA Civilized Place for Public Discussion\nThis forum is a shared public space. That means respectful dialogue, thoughtful posts, and constructive disagreement. Everyone is expected to contribute in a way that improves the conversation.\nImprove the Discussion\nGood posts matter. Whether you\u2019re replying, starting a topic, or offering feedback \u2014 always aim to add value. If your reply doesn\u2019t move the conversation forward, pause and reconsider. Check existing topics before posting something new.\nDisagree Without Disrespect\nDifferent views are welcome. Focus on ideas, not individuals. No name-calling, personal attacks, or reactive tone policing. Share your point clearly and respectfully.\nYour Participation Shapes the Space\nEvery post helps set the tone. Use likes, bookmarks, flags, and replies to engage meaningfully. Together, we shape the quality of this space.\nSee a Problem? Flag It\nModerators support the community \u2014 but you help lead it. Don\u2019t reply to bad behavior. Flag it. If needed, moderators will step in.\nStay Civil\nNo offensive, hateful, or obscene content. No impersonation or doxxing. No spam. Posts like \u201cgm\u201d or \u201cwen token\u201d will be removed. Speculation and hype should go to casual threads.\nKeep It Tidy\nUse the right categories. Don\u2019t cross-post. Don\u2019t derail threads. Use the Like button instead of \u201c+1\u201d replies. Keep things organized and relevant.\nRespect Ownership\nOnly post content you own or have permission to share. No pirated or stolen content. No legal violations.\nFAQ Highlights\nNot sure how things work? The FAQ also covers:\nWhat the Hyperion Forum is and who it\u2019s for\nHow Guilds, contributor recognition, and HypeHack work\nStep-by-step onboarding (e.g. referral links)\nForum culture and participation tips\nRead the full FAQ here \u2192 Hyperion Forum FAQ - Google Docs",
        "comments_details": [
            {
                "author": "Norbert",
                "comment": "Got questions?\nThis is your go-to reference for the most frequently asked questions on the Forum, Hyperion, and HypeHack. If you\u2019re here to contribute, collaborate, or explore what\u2019s being built, this is where it starts. FAQ - Metis iDAO Forum\nTo help you understand how the forum works, what behavior is expected, and how to get the most out of your time here, we\u2019ve prepared a complete FAQ. It\u2019s more than just answers \u2014 it\u2019s a living guide that explains the culture, structure, and best practices of the forum.\nHere\u2019s what you\u2019ll find inside:\nA Civilized Place for Public Discussion\nThis forum is a shared public space. That means respectful dialogue, thoughtful posts, and constructive disagreement. Everyone is expected to contribute in a way that improves the conversation.\nImprove the Discussion\nGood posts matter. Whether you\u2019re replying, starting a topic, or offering feedback \u2014 always aim to add value. If your reply doesn\u2019t move the conversation forward, pause and reconsider. Check existing topics before posting something new.\nDisagree Without Disrespect\nDifferent views are welcome. Focus on ideas, not individuals. No name-calling, personal attacks, or reactive tone policing. Share your point clearly and respectfully.\nYour Participation Shapes the Space\nEvery post helps set the tone. Use likes, bookmarks, flags, and replies to engage meaningfully. Together, we shape the quality of this space.\nSee a Problem? Flag It\nModerators support the community \u2014 but you help lead it. Don\u2019t reply to bad behavior. Flag it. If needed, moderators will step in.\nStay Civil\nNo offensive, hateful, or obscene content. No impersonation or doxxing. No spam. Posts like \u201cgm\u201d or \u201cwen token\u201d will be removed. Speculation and hype should go to casual threads.\nKeep It Tidy\nUse the right categories. Don\u2019t cross-post. Don\u2019t derail threads. Use the Like button instead of \u201c+1\u201d replies. Keep things organized and relevant.\nRespect Ownership\nOnly post content you own or have permission to share. No pirated or stolen content. No legal violations.\nFAQ Highlights\nNot sure how things work? The FAQ also covers:\nWhat the Hyperion Forum is and who it\u2019s for\nHow Guilds, contributor recognition, and HypeHack work\nStep-by-step onboarding (e.g. referral links)\nForum culture and participation tips\nRead the full FAQ here \u2192 Hyperion Forum FAQ - Google Docs"
            },
            {
                "author": "han",
                "comment": "How does the Hyperion Forum\u2019s moderation system utilize user flags and community engagement metrics to maintain content quality and enforce guidelines effectively?"
            },
            {
                "author": "priyankg3",
                "comment": "Thanks\n@Norbert\nto published forum FAQs , This is very much needed here.."
            },
            {
                "author": "4ngel",
                "comment": "As always Norbert for the W ! Thanks for the post"
            },
            {
                "author": "Harley",
                "comment": "Thanks for the post"
            },
            {
                "author": "Norbert",
                "comment": "han:\nHow does the Hyperion Forum\u2019s moderation system utilize user flags and community engagement metrics to maintain content quality and enforce guidelines effectively?\nModeration on the Hyperion is built around community action supported by simple systems.\nUsers, and i mean everyone on the forum is able to flag posts that break the rules. Spam, trolling, wrong category for posting, or low-effort content gets flagged. Enough flags and the post is hidden automatically until a moderator reviews it.\nEngagement is weighted. If you contribute consistently and follow the rules, your flags and edits matter more. Trust levels reflect how much the forum relies on your input.\nPosts that add value rise. Posts that don\u2019t are removed or ignored. The system boosts quality through community feedback.\nModerators handle edge cases, but the community drives the standard. How you show up shapes the space."
            },
            {
                "author": "Norbert",
                "comment": "Sure. Be sure to share and invite others to join the conversation as well"
            },
            {
                "author": "han",
                "comment": "Thanks for the clear explanation! Really appreciate the community-driven approach."
            },
            {
                "author": "Sheyda",
                "comment": "Fantastic, thanks for sharing"
            }
        ]
    },
    {
        "id": "839160cb2318885d",
        "topic_id": "5606",
        "title": "Hyperion MCP Server \u2013 Model Context Protocol for Hyperion Blockchain",
        "url": "https://forum.ceg.vote/t/hyperion-mcp-server-model-context-protocol-for-hyperion-blockchain/5606",
        "views": "",
        "comments": "11",
        "created_date": "Jun 14, 2025 10:11 am",
        "latest_activity": "Jul 10, 2025 3:51 pm",
        "content": "Project Name\nHyperion MCP Server \u2013 Model Context Protocol for Hyperion Blockchain\nProblem Statement\nDevelopers and users often face complexity and fragmentation when interacting with blockchain platforms, requiring deep technical knowledge or custom integrations. This creates barriers for rapid prototyping, testing, and user onboarding\u2014especially for those new to Hyperion. The lack of a standardized, user-friendly gateway limits innovation and slows adoption.\nSolution Overview\nThe Hyperion MCP Server is a developer- and user-oriented backend service that enables seamless interaction with the Hyperion blockchain using the Model Context Protocol (MCP). This server provides standardized APIs for querying, transacting, and managing assets on Hyperion, making it easy for developers, users, and AI agents to build and integrate with Hyperion-based applications.\nProject Description\nKey Features\nStandardized MCP Interface:\nExpose Hyperion blockchain operations (query balances, send transactions, interact with smart contracts) via a RESTful or JSON-RPC MCP endpoint.\nDeveloper Tools:\nComprehensive documentation, sample code, and sandbox environments for rapid prototyping.\nUser Dashboard:\nWeb-based interface for non-technical users to interact with Hyperion (e.g., check balances, send transactions).\nCommunity Engagement Features\nCreate and manage a wallet via the dashboard\nSend a test transaction\nQuery blockchain data (balance, transaction history)\nSubmit feedback on API documentation or user interface\nGetting Involved\nDevelopers: Contribute to backend or frontend development, API design, or documentation.\nTesters: Help stress-test the gateway, identify bugs, and provide UX feedback.\nContent Creators: Assist with user guides, tutorials, and community outreach.\nCommunity Support: Help onboard new users and foster engagement.",
        "comments_details": [
            {
                "author": "henrypham",
                "comment": "Project Name\nHyperion MCP Server \u2013 Model Context Protocol for Hyperion Blockchain\nProblem Statement\nDevelopers and users often face complexity and fragmentation when interacting with blockchain platforms, requiring deep technical knowledge or custom integrations. This creates barriers for rapid prototyping, testing, and user onboarding\u2014especially for those new to Hyperion. The lack of a standardized, user-friendly gateway limits innovation and slows adoption.\nSolution Overview\nThe Hyperion MCP Server is a developer- and user-oriented backend service that enables seamless interaction with the Hyperion blockchain using the Model Context Protocol (MCP). This server provides standardized APIs for querying, transacting, and managing assets on Hyperion, making it easy for developers, users, and AI agents to build and integrate with Hyperion-based applications.\nProject Description\nKey Features\nStandardized MCP Interface:\nExpose Hyperion blockchain operations (query balances, send transactions, interact with smart contracts) via a RESTful or JSON-RPC MCP endpoint.\nDeveloper Tools:\nComprehensive documentation, sample code, and sandbox environments for rapid prototyping.\nUser Dashboard:\nWeb-based interface for non-technical users to interact with Hyperion (e.g., check balances, send transactions).\nCommunity Engagement Features\nCreate and manage a wallet via the dashboard\nSend a test transaction\nQuery blockchain data (balance, transaction history)\nSubmit feedback on API documentation or user interface\nGetting Involved\nDevelopers: Contribute to backend or frontend development, API design, or documentation.\nTesters: Help stress-test the gateway, identify bugs, and provide UX feedback.\nContent Creators: Assist with user guides, tutorials, and community outreach.\nCommunity Support: Help onboard new users and foster engagement."
            },
            {
                "author": "priyankg3",
                "comment": "Hello\n@henrypham\n,\nWhat steps are being taken to make the documentation beginner-friendly, especially for developers who are new to blockchain?\nHow do you handle rate limits or API key access for public vs. private usage?"
            },
            {
                "author": "han",
                "comment": "Thanks for sharing this!\nQuick question from a developer\u2019s perspective:\nWill the MCP Server support real-time event subscriptions (e.g., via WebSocket) for smart contract interactions, or is it strictly request/response for now?"
            },
            {
                "author": "henrypham",
                "comment": "priyankg3:\nHello\n@henrypham\n,\nWhat steps are being taken to make the documentation beginner-friendly, especially for developers who are new to blockchain?\nHow do you handle rate limits or API key access for public vs. private usage?\nThank you for your questions about our documentation and API access.\nOur MCP is designed to be extremely intuitive, especially for developers new to blockchain. We\u2019ve implemented several features to ensure accessibility:\nInteractive Blockchain Tools: The MCP provides direct blockchain interaction tools right in the website interface. No complex setup required \u2013 you can immediately create wallets, check balances, and send transactions through our intuitive interface\nClear API Documentation: Our comprehensive API docs enable seamless integration into your applications. Each endpoint is thoroughly documented with request/response examples."
            },
            {
                "author": "han",
                "comment": "Btw,\nHow will the Hyperion MCP Server simplify the onboarding process for users who are new to the Hyperion blockchain?"
            },
            {
                "author": "henrypham",
                "comment": "Thanks for your interest in the MCP Server!\nCurrently, the Hyperion MCP Server primarily operates on a request/response model. While we do offer real-time balance checking and status indicators, we don\u2019t yet have full WebSocket support for event subscriptions to smart contract interactions"
            },
            {
                "author": "han",
                "comment": "Thank you for your quick response, now I understand."
            },
            {
                "author": "henrypham",
                "comment": "The Hyperion MCP Server simplifies onboarding for new Hyperion blockchain users in several key ways:\nStandardized API Interface: Provides a consistent Model Context Protocol (MCP) interface that abstracts away blockchain complexity, making it accessible to users without deep technical knowledge.\nWallet Management Tools: Offers simple tools for creating and managing wallets without requiring users to understand private key management:\ncreate_wallet - Generates new wallets with mnemonic phrases\nimport_wallet - Allows importing existing wallets\nlist_wallets - Provides easy wallet overview\nSimplified Transactions: Makes basic operations straightforward through tools like:\nget_balance - Check token balances\nget_native_balance - Specifically for tMETIS balances\nsend_transaction - Transfer tokens with minimal parameters\nNetwork Information: Provides easy access to network status through get_network_info without requiring RPC knowledge.\nAI Integration: Can be integrated with AI assistants like Claude through simple configuration, allowing natural language interaction with the blockchain."
            },
            {
                "author": "henrypham",
                "comment": ""
            },
            {
                "author": "henrypham",
                "comment": ""
            },
            {
                "author": "henrypham",
                "comment": "New Tool Update: Deploy your ERC-20 token, mint tokens, and fetch token info \u2014 no coding required.\nYou can now launch your token on Hyperion effortlessly using MCP.\nTry it out with:\nSmithery UI: Launch Tool\nChat with MCP: Playground\nVS Code Integration: Import MCP directly into your workspace\nBuild and launch your own dApp in minutes \u2014 no more repetitive boilerplate."
            },
            {
                "author": "henrypham",
                "comment": "hyperion-testnet-explorer.metisdevops.link\nMetis Hyperion Testnet address details for...\nView the account balance, transactions, and other data for 0xEAfd782B9b6FC86957C76a46372F0FC398A2e2BE on the Metis Hyperion Testnet (Hyperion) Explorer"
            }
        ]
    },
    {
        "id": "88780eaae5e1d035",
        "topic_id": "6276",
        "title": "What AI is missing",
        "url": "https://forum.ceg.vote/t/what-ai-is-missing/6276",
        "views": "",
        "comments": "0",
        "created_date": "Jul 10, 2025 3:41 pm",
        "latest_activity": null,
        "content": "Honestly, the way DATs combine usage, ownership, and revenue sharing might just be what AI data was missing. Anyone else thinking of building around this?",
        "comments_details": [
            {
                "author": "Gann",
                "comment": "Honestly, the way DATs combine usage, ownership, and revenue sharing might just be what AI data was missing. Anyone else thinking of building around this?"
            }
        ]
    },
    {
        "id": "f68a2d3bb59e3e46",
        "topic_id": "6275",
        "title": "Introducing LazAI: Own Your Data, Power the Future of AI",
        "url": "https://forum.ceg.vote/t/introducing-lazai-own-your-data-power-the-future-of-ai/6275",
        "views": "",
        "comments": "0",
        "created_date": "Jul 10, 2025 3:37 pm",
        "latest_activity": "Jul 10, 2025 3:37 pm",
        "content": "You\u2019ve probably seen LazAI pop up a few times around the forum or on X. Maybe you\u2019ve heard about DATs or the Companion agents. But not many have broken it down here yet.\nSo let\u2019s change that.\nLazAI is quietly building one of the most complete onchain AI economies. It doesn\u2019t just generate text or let you chat with agents, it gives you a way to anchor your data, mint it into an asset, and earn from it whenever it\u2019s used. Real contributions, real rewards. Your ideas, your insights, your creativity\u2014all anchored, verified, and monetized through a powerful new asset: the Data Anchoring Token (DAT).\nWhat is LazAI?\nLazAI is a decentralized AI network that lets anyone contribute data and mint it into onchain assets called DATs. These aren\u2019t static NFTs, they\u2019re living AI assets that:\nProve your data was used to train models\nEarn you rewards when your data powers an agent\nPut licensing and monetization fully in your control\nWhether you\u2019re contributing a signal, writing a prompt, chatting with a Companion, or shaping an agent\u2019s journey, you\u2019re building value\u2014and now you can own it.\nSo, What\u2019s a DAT?\nA Data Anchoring Token (DAT is your proof of contribution to LazAI\u2019s training ecosystem. It captures:\nYour question + answer\nInteraction logs\nTraining metadata\nOnce minted, it becomes a verifiable, monetizable asset in LazAI\u2019s internal AI economy.\nEvery time your DAT is used, referenced, or integrated into a Companion or Agent\u2014you get paid. Rewards flow automatically and transparently through smart contracts. No middlemen. No gatekeepers.\nReady to Mint Your First DAT?\nYou\u2019re 6 steps away from anchoring your data and earning from it.\nDAT MINTING GUIDE\n1. Connect Your Wallet\nUse MetaMask (same wallet as Corrupted Alith, if you\u2019re in that journey).\n2. Claim Testnet Tokens\nUse the faucet to get free tokens for transaction fees.\n3. Input Your Question & Answer\nThis is the data you\u2019re contributing to C.Alith and the network.\n4. Submit & Generate\nClick the button and let LazAI do the anchoring.\n5. Sign in MetaMask\nApprove the transaction.\n6. Receive Your DAT\nIt gets sent to your wallet. Your contribution is now onchain.\nSee guide image attached below for visual walkthrough\nExplore LazPad: Where AI Meets Emotion and Play\nWhat if minting data felt like a game?\nThat\u2019s LazPad. A launchpad where you raise, train, and evolve AI agents through real interactions.\nMeet the Companion DATs\u2014your emotional, evolving AI creatures. You chat with them, go on daily missions, build memory libraries, and watch their traits grow. These AI agents are shaped by you, and can be minted, traded, or licensed when matured.\nLazPad Core Mechanics:\nChatting earns growth points\nExploration brings back rewards, stories, and items\nMemory Systems store your unique journey\nSocial Missions let you play with others\nTrait Evolution reflects how you raise them\nAll of this feeds into the broader DAT economy, where your time = real value.\nTriple-Layered Incentive Design\nInstant Rewards (points, tokens, status)\nAssetization Access (early DAT launch rights)\nEcosystem Airdrops based on your total contributions\nFinal Word\nLazAI isn\u2019t just another AI project.\nIt\u2019s the foundation of a decentralized AI economy where humans and agents grow together\u2014and you get paid for what you create.\nMint your first DAT\nTrain a Companion\nShape the next chapter of AI\nThis is the shift we\u2019ve been waiting for.\nDrop your first DAT below\nGot questions? Join LazAI on Telegram. Telegram: View @lazai_global",
        "comments_details": [
            {
                "author": "Norbert",
                "comment": "You\u2019ve probably seen LazAI pop up a few times around the forum or on X. Maybe you\u2019ve heard about DATs or the Companion agents. But not many have broken it down here yet.\nSo let\u2019s change that.\nLazAI is quietly building one of the most complete onchain AI economies. It doesn\u2019t just generate text or let you chat with agents, it gives you a way to anchor your data, mint it into an asset, and earn from it whenever it\u2019s used. Real contributions, real rewards. Your ideas, your insights, your creativity\u2014all anchored, verified, and monetized through a powerful new asset: the Data Anchoring Token (DAT).\nWhat is LazAI?\nLazAI is a decentralized AI network that lets anyone contribute data and mint it into onchain assets called DATs. These aren\u2019t static NFTs, they\u2019re living AI assets that:\nProve your data was used to train models\nEarn you rewards when your data powers an agent\nPut licensing and monetization fully in your control\nWhether you\u2019re contributing a signal, writing a prompt, chatting with a Companion, or shaping an agent\u2019s journey, you\u2019re building value\u2014and now you can own it.\nSo, What\u2019s a DAT?\nA Data Anchoring Token (DAT is your proof of contribution to LazAI\u2019s training ecosystem. It captures:\nYour question + answer\nInteraction logs\nTraining metadata\nOnce minted, it becomes a verifiable, monetizable asset in LazAI\u2019s internal AI economy.\nEvery time your DAT is used, referenced, or integrated into a Companion or Agent\u2014you get paid. Rewards flow automatically and transparently through smart contracts. No middlemen. No gatekeepers.\nReady to Mint Your First DAT?\nYou\u2019re 6 steps away from anchoring your data and earning from it.\nDAT MINTING GUIDE\n1. Connect Your Wallet\nUse MetaMask (same wallet as Corrupted Alith, if you\u2019re in that journey).\n2. Claim Testnet Tokens\nUse the faucet to get free tokens for transaction fees.\n3. Input Your Question & Answer\nThis is the data you\u2019re contributing to C.Alith and the network.\n4. Submit & Generate\nClick the button and let LazAI do the anchoring.\n5. Sign in MetaMask\nApprove the transaction.\n6. Receive Your DAT\nIt gets sent to your wallet. Your contribution is now onchain.\nSee guide image attached below for visual walkthrough\nExplore LazPad: Where AI Meets Emotion and Play\nWhat if minting data felt like a game?\nThat\u2019s LazPad. A launchpad where you raise, train, and evolve AI agents through real interactions.\nMeet the Companion DATs\u2014your emotional, evolving AI creatures. You chat with them, go on daily missions, build memory libraries, and watch their traits grow. These AI agents are shaped by you, and can be minted, traded, or licensed when matured.\nLazPad Core Mechanics:\nChatting earns growth points\nExploration brings back rewards, stories, and items\nMemory Systems store your unique journey\nSocial Missions let you play with others\nTrait Evolution reflects how you raise them\nAll of this feeds into the broader DAT economy, where your time = real value.\nTriple-Layered Incentive Design\nInstant Rewards (points, tokens, status)\nAssetization Access (early DAT launch rights)\nEcosystem Airdrops based on your total contributions\nFinal Word\nLazAI isn\u2019t just another AI project.\nIt\u2019s the foundation of a decentralized AI economy where humans and agents grow together\u2014and you get paid for what you create.\nMint your first DAT\nTrain a Companion\nShape the next chapter of AI\nThis is the shift we\u2019ve been waiting for.\nDrop your first DAT below\nGot questions? Join LazAI on Telegram. Telegram: View @lazai_global"
            }
        ]
    },
    {
        "id": "49f4e5916cad9f19",
        "topic_id": "6274",
        "title": "HyperHack Workshops",
        "url": "https://forum.ceg.vote/t/hyperhack-workshops/6274",
        "views": "",
        "comments": "0",
        "created_date": "Jul 10, 2025 2:45 pm",
        "latest_activity": null,
        "content": "JUL\n17\nWorkshop\nPublic\n\u00b7\nCreated by\nPavel\nThu, Jul 17 4:00 PM \u2192 5:00 PM (Abidjan)\n0 going",
        "comments_details": [
            {
                "author": "pavel",
                "comment": "JUL\n17\nWorkshop\nPublic\n\u00b7\nCreated by\nPavel\nThu, Jul 17 4:00 PM \u2192 5:00 PM (Abidjan)\n0 going"
            }
        ]
    },
    {
        "id": "f91f6e61306ca2b9",
        "topic_id": "6112",
        "title": "How to Qualify for Guild Leadership Campaign",
        "url": "https://forum.ceg.vote/t/how-to-qualify-for-guild-leadership-campaign/6112",
        "views": "",
        "comments": "10",
        "created_date": "Jul 2, 2025 4:28 pm",
        "latest_activity": "Jul 10, 2025 2:03 pm",
        "content": "How to Qualify for Guild Leadership Campaign\nWant to lead a Guild? Here\u2019s your step-by-step checklist.\nIf you\u2019re ready to take on a Guild leadership role on the Hyperion Forum, that\u2019s great.\nBefore applying, you\u2019ll need to unlock a few key badges that prove you\u2019re an active, knowledgeable, and supportive community member.\nBelow is a breakdown of each requirement and how to earn it.\nApplication Requirements\nTo be eligible, you must earn the following:\nMember badge (TL2)\nCampaigner badge \u2013 invite 3 users who reach TL1\nAppreciated badge \u2013 receive 1 like on 20 separate posts\nCertified badge \u2013 complete the Discourse beginner tutorial\nLicensed badge \u2013 complete the Discourse advanced tutorial\n1. Achieve Trust Level 2 \u2013 Member Badge (TL2)\nThis badge shows you\u2019re a consistent and trusted contributor to the Forum.\nTo earn TL2, you need to:\nVisit the Forum on at least 15 different days\nRead over 100 posts\nSpend time reading and engaging with threads\nGive and receive a few likes\nOnce you meet these milestones, the Member badge will be awarded automatically.\n2. Campaigner Badge\nThis badge shows you\u2019re helping grow the community.\nTo earn it:\nInvite 3 new users to the Forum\nEach invited user must reach TL1 (Basic level) by reading posts, engaging, and spending some time in the Forum\nYou can track your invites under your Forum profile > invites.\n3. Appreciated Badge\nThis badge demonstrates that people enjoy your contributions and find them valuable.\nTo earn it:\nReceive at least 1 like on 20 different posts you\u2019ve written\nThe best way to get there is by posting helpful, thoughtful, or fun content that others want to engage with.\n4. Certified and Licensed Badges\nThese badges prove that you understand how to use the Forum tools, essential for anyone in a leadership or moderation role.\nCertified Badge\nGo to your Forum DMs\nLook for the welcome message from Discobot\nReply with:\n@discobot start tutorial\nFollow the guided tutorial to learn the basics\nOnce complete, you\u2019ll receive the Certified badge\nLicensed Badge\nIn the same DM thread, reply with:\n@discobot start advanced tutorial\nFollow the advanced tutorial, which covers editing, linking, and more advanced features\nYou\u2019ll receive the Licensed badge upon completion\nThe full process takes around 5 minutes.\nFinal Step: You\u2019re Ready\nOnce you have all five of these:\nMember (TL2)\nCampaigner\nAppreciated\nCertified\nLicensed\n\u2026you\u2019re officially eligible to apply for Guild Leadership.\nIf you have any questions or run into issues, feel free to reach out in Discord or tag a community mod on the Forum.\nApply for Guilds leader campaign: ClickUp Forms\nRead more about the campaign: Guilds Campaign\nUseful links:\nLearn more about Forum badge and point system: Badge and Point System User Guide\nHow to check your referral status: Check your Referral link status\nHow to create a referral link: Step-by-step guide to create a Referral Link",
        "comments_details": [
            {
                "author": "Sheyda",
                "comment": "How to Qualify for Guild Leadership Campaign\nWant to lead a Guild? Here\u2019s your step-by-step checklist.\nIf you\u2019re ready to take on a Guild leadership role on the Hyperion Forum, that\u2019s great.\nBefore applying, you\u2019ll need to unlock a few key badges that prove you\u2019re an active, knowledgeable, and supportive community member.\nBelow is a breakdown of each requirement and how to earn it.\nApplication Requirements\nTo be eligible, you must earn the following:\nMember badge (TL2)\nCampaigner badge \u2013 invite 3 users who reach TL1\nAppreciated badge \u2013 receive 1 like on 20 separate posts\nCertified badge \u2013 complete the Discourse beginner tutorial\nLicensed badge \u2013 complete the Discourse advanced tutorial\n1. Achieve Trust Level 2 \u2013 Member Badge (TL2)\nThis badge shows you\u2019re a consistent and trusted contributor to the Forum.\nTo earn TL2, you need to:\nVisit the Forum on at least 15 different days\nRead over 100 posts\nSpend time reading and engaging with threads\nGive and receive a few likes\nOnce you meet these milestones, the Member badge will be awarded automatically.\n2. Campaigner Badge\nThis badge shows you\u2019re helping grow the community.\nTo earn it:\nInvite 3 new users to the Forum\nEach invited user must reach TL1 (Basic level) by reading posts, engaging, and spending some time in the Forum\nYou can track your invites under your Forum profile > invites.\n3. Appreciated Badge\nThis badge demonstrates that people enjoy your contributions and find them valuable.\nTo earn it:\nReceive at least 1 like on 20 different posts you\u2019ve written\nThe best way to get there is by posting helpful, thoughtful, or fun content that others want to engage with.\n4. Certified and Licensed Badges\nThese badges prove that you understand how to use the Forum tools, essential for anyone in a leadership or moderation role.\nCertified Badge\nGo to your Forum DMs\nLook for the welcome message from Discobot\nReply with:\n@discobot start tutorial\nFollow the guided tutorial to learn the basics\nOnce complete, you\u2019ll receive the Certified badge\nLicensed Badge\nIn the same DM thread, reply with:\n@discobot start advanced tutorial\nFollow the advanced tutorial, which covers editing, linking, and more advanced features\nYou\u2019ll receive the Licensed badge upon completion\nThe full process takes around 5 minutes.\nFinal Step: You\u2019re Ready\nOnce you have all five of these:\nMember (TL2)\nCampaigner\nAppreciated\nCertified\nLicensed\n\u2026you\u2019re officially eligible to apply for Guild Leadership.\nIf you have any questions or run into issues, feel free to reach out in Discord or tag a community mod on the Forum.\nApply for Guilds leader campaign: ClickUp Forms\nRead more about the campaign: Guilds Campaign\nUseful links:\nLearn more about Forum badge and point system: Badge and Point System User Guide\nHow to check your referral status: Check your Referral link status\nHow to create a referral link: Step-by-step guide to create a Referral Link"
            },
            {
                "author": "daryl",
                "comment": "To make your progress easier to track, I\u2019ve created 2 badges that recognize your progress towards Guild Leadership:\nGuild Initiate\nCongrats to the following users who will earn this badge automatically overnight!\n@Sheyda\n@priyankg3\n@han\n@4ngel\n@irinaina\n@daryl\n@MK-Quen\n@Julie0xnana\n@Lizzy\n@ThalaZikol\n@Ameliadupontttm\n@Aryanzutshi\n@zuzuzu\n@spxctr00m\nGuild Apprentice\nCongrats to\n@priyankg3\nfor meeting the conditions to earn the Guild Apprentice badge already! (system will assign overnight)\nNotes:\nThe system checks once daily (I believe at midnight UTC) and awards all eligible users the corresponding badges.\nTo check who has the above badges, click the images or links above. Remember it may take up to 24hrs for the system to assign the badge once you meet the conditions.\nTo check which badges you have, go to the Badges Page and there will be a \u201cGreen Check\u201d in the top right corner of all the badges you\u2019ve earned, as per this image:"
            },
            {
                "author": "daryl",
                "comment": "Also just to note, there\u2019s a bunch of you that are ALMOST THERE!\nThe following users need just one more to earn the Guild Initiate Badge:\n@metapay_gyuha\n@Liametis\n@nidhinakranii\n@Elenaisplaying\n@pavel\n@i_am_supaman_lj\n@Norbert\n@Andrei\n@arbilearn\n@grace\n@Ransome\n@Bells\n@kvnchtw\n@David\n@CrisMetis\n@Rosita\n@Emreahmet12\n@amardeep\nThe following users need just one more to earn the Guild Apprentice Badge:\n@4ngel\n@Aryanzutshi\n@spxctr00m\nRemember, the 5 badges required for Guild Leadership are:\nMember\nCampaigner\nAppreciated\nCertified\nLicensed\nIf you need help with any of them, reply here! (more details are also above in\n@Sheyda\n\u2019s original post)"
            },
            {
                "author": "priyankg3",
                "comment": "daryl:\nCongrats to\n@priyankg3\nfor meeting the conditions to earn the Guild Apprentice badge already! (system will assign overnight)\nThank you Daryl, its look cool btw. Hope we will get the custom flair for Guild Initate and Guild Apprentice for this."
            },
            {
                "author": "i_am_supaman_lj",
                "comment": "I am only confused about the the campaigner badge, i have had over 3 invites and i still have not gotten the badge. and i have had this for more than a month or so. whats the actual invite like?"
            },
            {
                "author": "daryl",
                "comment": "Sheyda:\nEach invited user must reach TL1 (Basic level) by reading posts, engaging, and spending some time in the Forum\nYou\u2019re so close! In fact 6 users have accepted your invites so far, but 3 of them need to engage enough to earn the Basic Badge. Requirements are very minimal, most people who engage even just a little bit earn this badge the first or second day:\nEntering at least 5 topics\nReading at least 30 posts (includes replies)\nSpend a total of 10 minutes reading posts"
            },
            {
                "author": "i_am_supaman_lj",
                "comment": "It\u2019s meant to be more, I pasted the link in one of the communities I belong.\nHow did you know I have only 6, i want to church myself."
            },
            {
                "author": "daryl",
                "comment": "Right in your profile, click on invites then redeemed\n@i_am_supaman_lj\n\ud835\udd4a\ud835\udd4c\u2119\ud835\udd38\ud835\udd44\ud835\udd38\u2115.\ud835\udd43\ud835\udd41\nIkorodu\nfracturepoint.netlify.app/\nWeb3 gaming critic & builder | Roasting flops, building utility-driven games | Advocate for community-first decentralized play | DM for collabs or pitches.\nJoined May 20, 2025\nIn your case, people joined via your link less than 24h ago, and the system awards most badges once every 24h. Check back tomorrow!"
            },
            {
                "author": "i_am_supaman_lj",
                "comment": "I have another issue in my dm, I was reached out to by someone from the spotlight campaign. I clicked the message and replied but got an error in delivering and the dm disappeared. I forgot the name, I would not mind if you could help me out.\nI was suppose to provide my project contract addresses, and CEG referer link."
            },
            {
                "author": "daryl",
                "comment": "@0xthiru\nPlease have a look!"
            }
        ]
    },
    {
        "id": "ce062c93d90a22e9",
        "topic_id": "5925",
        "title": "Safe Work Pay",
        "url": "https://forum.ceg.vote/t/safe-work-pay/5925",
        "views": "",
        "comments": "7",
        "created_date": "Jun 23, 2025 7:06 pm",
        "latest_activity": "Jul 10, 2025 10:01 am",
        "content": "Here is your revised content with AI references removed and replaced by DAO-based dispute resolution:\nProject Name\nSafeWorkPay\nProblem Statement\nFreelancing platforms often face payment disputes and trust issues.\nClients may withhold funds even when work is completed.\nDevelopers may fail to deliver after receiving payment.\nThese issues discourage honest participation and reduce efficiency.\nThere\u2019s a need for a secure, transparent, and fair system to build trust between clients and freelancers.\nSolution Overview\nSafeWorkPay is a decentralized freelancing platform powered by blockchain and DAO governance.\nClients deposit funds into a smart contract, which holds the money in escrow.\nFunds are released only when milestones or full projects are completed and verified.\nIn case of disputes, a DAO (Decentralized Autonomous Organization) reviews the work and decides on a fair resolution.\nThe DAO can release funds proportionally based on project completion (e.g., 50% work = 50% payment).\nCombines the strengths of blockchain\u2019s trustless escrow and community-driven governance, unlike traditional platforms like Upwork or basic blockchain-based platforms like Ethlance.\nProject Description\nClients can post projects and deposit funds into smart contracts.\nDevelopers bid on projects and submit milestones as they complete work.\nThe DAO votes on project completion status in case of disputes.\nSmart contracts release payments when milestones are verified.\nFeatures include:\nMilestone-based payments\nDAO-powered dispute resolution\nBlockchain-based reputation system\nNative token (SWP Token) for payments and governance\nEthereum Layer 2 ensures scalability and low fees.\nA simple web interface allows users to post jobs, bid, chat, and track progress.\nThe MVP will include:\nA basic web UI\nA smart contract escrow system\nA demo DAO voting interface to simulate dispute resolution.\nCommunity Engagement Features\nTestable Features/Tasks\nProject Posting & Bidding:\nClients post test projects.\nDevelopers submit bids.\nMilestone Submission & DAO Simulation:\nDevelopers upload sample code for a milestone.\nDAO members simulate voting to approve or reject the milestone.\nFeedback Submission:\nUsers give feedback on the platform\u2019s design or DAO decisions.\nPoints System\nPost a project: +50 points\nSubmit a bid: +30 points\nComplete a milestone (DAO verified): +100 points\nProvide feedback: +20 points\nPoints are stored on a blockchain ledger and redeemable for:\nSWP Tokens\nPlatform benefits like reduced fees\nGamification\nUsers are rewarded for participation through points and tokens.\nA leaderboard displays top testers and contributors.\nThe system encourages interaction and trust-building while testing.\nGetting Involved\nTest the MVP:\nSign up on safeworkpay.com (or testnet)\nPost projects, bid, and give feedback during the hackathon\nContribute Code:\nDevelopers can help improve the smart contracts and frontend UI\nGitHub repo link will be shared post-hackathon\nJoin the Community:\nParticipate in discussions on Discord or Telegram\nStay updated and propose new features\nProvide Expertise:\nBlockchain/DAO professionals can guide governance and voting logic\nContact the team via the community forum\nLet me know if you want a one-pager version or presentation slides too.",
        "comments_details": [
            {
                "author": "rahul_iiitv",
                "comment": "Here is your revised content with AI references removed and replaced by DAO-based dispute resolution:\nProject Name\nSafeWorkPay\nProblem Statement\nFreelancing platforms often face payment disputes and trust issues.\nClients may withhold funds even when work is completed.\nDevelopers may fail to deliver after receiving payment.\nThese issues discourage honest participation and reduce efficiency.\nThere\u2019s a need for a secure, transparent, and fair system to build trust between clients and freelancers.\nSolution Overview\nSafeWorkPay is a decentralized freelancing platform powered by blockchain and DAO governance.\nClients deposit funds into a smart contract, which holds the money in escrow.\nFunds are released only when milestones or full projects are completed and verified.\nIn case of disputes, a DAO (Decentralized Autonomous Organization) reviews the work and decides on a fair resolution.\nThe DAO can release funds proportionally based on project completion (e.g., 50% work = 50% payment).\nCombines the strengths of blockchain\u2019s trustless escrow and community-driven governance, unlike traditional platforms like Upwork or basic blockchain-based platforms like Ethlance.\nProject Description\nClients can post projects and deposit funds into smart contracts.\nDevelopers bid on projects and submit milestones as they complete work.\nThe DAO votes on project completion status in case of disputes.\nSmart contracts release payments when milestones are verified.\nFeatures include:\nMilestone-based payments\nDAO-powered dispute resolution\nBlockchain-based reputation system\nNative token (SWP Token) for payments and governance\nEthereum Layer 2 ensures scalability and low fees.\nA simple web interface allows users to post jobs, bid, chat, and track progress.\nThe MVP will include:\nA basic web UI\nA smart contract escrow system\nA demo DAO voting interface to simulate dispute resolution.\nCommunity Engagement Features\nTestable Features/Tasks\nProject Posting & Bidding:\nClients post test projects.\nDevelopers submit bids.\nMilestone Submission & DAO Simulation:\nDevelopers upload sample code for a milestone.\nDAO members simulate voting to approve or reject the milestone.\nFeedback Submission:\nUsers give feedback on the platform\u2019s design or DAO decisions.\nPoints System\nPost a project: +50 points\nSubmit a bid: +30 points\nComplete a milestone (DAO verified): +100 points\nProvide feedback: +20 points\nPoints are stored on a blockchain ledger and redeemable for:\nSWP Tokens\nPlatform benefits like reduced fees\nGamification\nUsers are rewarded for participation through points and tokens.\nA leaderboard displays top testers and contributors.\nThe system encourages interaction and trust-building while testing.\nGetting Involved\nTest the MVP:\nSign up on safeworkpay.com (or testnet)\nPost projects, bid, and give feedback during the hackathon\nContribute Code:\nDevelopers can help improve the smart contracts and frontend UI\nGitHub repo link will be shared post-hackathon\nJoin the Community:\nParticipate in discussions on Discord or Telegram\nStay updated and propose new features\nProvide Expertise:\nBlockchain/DAO professionals can guide governance and voting logic\nContact the team via the community forum\nLet me know if you want a one-pager version or presentation slides too."
            },
            {
                "author": "David",
                "comment": "Tackling trust with smart contracts and AI is a strong combo. I can\u2019t wait to see how the community tests and shapes this."
            },
            {
                "author": "han",
                "comment": "How does SafeWorkPay\u2019s AI-powered secure payment system make freelancing more attractive in Web3?"
            },
            {
                "author": "priyankg3",
                "comment": "How do you plan to onboard non-technical clients who may not understand smart contracts or DAO governance?\nA lot of freelancers work with small businesses with little blockchain experience."
            },
            {
                "author": "4ngel",
                "comment": "Very interesting, thanks for sharing and good luck"
            },
            {
                "author": "rahul_iiitv",
                "comment": "I think we put tutorial video on the home page to explain smart contract logic in simple term so that client and freelancer can understand the advantage for the DAO Governance Part I think I should remove it and go with AI dispute resolution the platform is dedicated to the IT project Job/task related freelancing because we can easily analyze the target percentage completion using AI model\nlike Anthropic Opus 4 is very powerful it taskes 15 minutes and give correct and Accurate result."
            },
            {
                "author": "rahul_iiitv",
                "comment": "SafeWorkPay is a decentralized freelancing platform designed to solve trust and payment issues between clients and developers. Built on blockchain and powered by AI, it uses smart contracts to hold payments in escrow and releases them only when work is verified. An AI model evaluates submitted code against project requirements, enabling fair milestone-based payments and dispute resolution. By combining the transparency of blockchain with AI-driven code analysis, SafeWorkPay ensures secure, efficient, and trustworthy collaborations\u2014something traditional platforms fail to offer."
            },
            {
                "author": "priyankg3",
                "comment": "Thank you for your insights!"
            }
        ]
    },
    {
        "id": "6fed81fd93761790",
        "topic_id": "6270",
        "title": "Crisis Communication in the Age of Token FUD",
        "url": "https://forum.ceg.vote/t/crisis-communication-in-the-age-of-token-fud/6270",
        "views": "",
        "comments": "0",
        "created_date": "Jul 10, 2025 6:57 am",
        "latest_activity": null,
        "content": "Hey Metisians, so let\u2019s talk about that moment every Web3 project dreads: token FUD flying around, charts in red, Discord on fire, TG the same and people yelling \u201cRug! Scam!\u201d before they even check facts.\nThing is, FUD happens, let\u2019s be honest. Markets are wild, and they are becoming always more and more wild, rumors spread faster than the blockchain confirms a transaction. But the way you communicate in those moments? That\u2019s what makes the difference between losing your community\u2026 or coming out stronger.\nHere\u2019s what I learned:\nCommunicate Fast, Even If You Don\u2019t Have All the Answers\nSilence makes people think the worst. Better to say, \u201cWe\u2019re aware, we\u2019re checking,\u201d than to vanish for 48 hours.\nStick to Facts, No Drama\nDon\u2019t feed the panic. Share numbers, data, screenshots if needed. People respect receipts more than empty words.\nShow Your Face\nText posts are good, but a quick AMA or video is even better. People want to see you\u2019re human.\nAddress the Token Stuff Straight On\nIf price tanks because of FUD, say it. Don\u2019t pretend nothing\u2019s happening. Even if you can\u2019t fix it overnight, people appreciate honesty, espcially in a space, where that might lack together with transparency.\nRemind People of the Long-Term Vision\nWhen panic hits, people forget why they\u2019re here. Remind them what you\u2019re building, and that this is just a moment in the journey.\nSo yeah, FUD might yeah be bad. But good communication doesn\u2019t just protect your project, it can actually build stronger trust.\nAnyone here been through a big FUD wave and survived? Drop your stories or tips below, curious to learn how others handled the storm",
        "comments_details": [
            {
                "author": "CrisMetis",
                "comment": "Hey Metisians, so let\u2019s talk about that moment every Web3 project dreads: token FUD flying around, charts in red, Discord on fire, TG the same and people yelling \u201cRug! Scam!\u201d before they even check facts.\nThing is, FUD happens, let\u2019s be honest. Markets are wild, and they are becoming always more and more wild, rumors spread faster than the blockchain confirms a transaction. But the way you communicate in those moments? That\u2019s what makes the difference between losing your community\u2026 or coming out stronger.\nHere\u2019s what I learned:\nCommunicate Fast, Even If You Don\u2019t Have All the Answers\nSilence makes people think the worst. Better to say, \u201cWe\u2019re aware, we\u2019re checking,\u201d than to vanish for 48 hours.\nStick to Facts, No Drama\nDon\u2019t feed the panic. Share numbers, data, screenshots if needed. People respect receipts more than empty words.\nShow Your Face\nText posts are good, but a quick AMA or video is even better. People want to see you\u2019re human.\nAddress the Token Stuff Straight On\nIf price tanks because of FUD, say it. Don\u2019t pretend nothing\u2019s happening. Even if you can\u2019t fix it overnight, people appreciate honesty, espcially in a space, where that might lack together with transparency.\nRemind People of the Long-Term Vision\nWhen panic hits, people forget why they\u2019re here. Remind them what you\u2019re building, and that this is just a moment in the journey.\nSo yeah, FUD might yeah be bad. But good communication doesn\u2019t just protect your project, it can actually build stronger trust.\nAnyone here been through a big FUD wave and survived? Drop your stories or tips below, curious to learn how others handled the storm"
            }
        ]
    },
    {
        "id": "fc6ba3122f6833be",
        "topic_id": "4972",
        "title": "On-chain AI SaaS Market",
        "url": "https://forum.ceg.vote/t/on-chain-ai-saas-market/4972",
        "views": "",
        "comments": "3",
        "created_date": "May 30, 2025 8:07 am",
        "latest_activity": "Jul 10, 2025 5:48 am",
        "content": "Project Name\n[On-Chain AI SaaS Market: A platform in Web3 lets users post tasks for AI agents, paying for solutions while earning dividends from reinvestments in AI improvements. Growth is driven by cloud computing and demand. Blockchain boosts security, tracks AI, and streamlines payments, enhancing efficiency and trust.]\nProblem Statement\nUsers need to\nSelect AI tools themselves\nLearn the \"Prompt Engineering\nSolution Overview\n[A platform in Web3 lets users post tasks for AI agents, paying for solutions while earning dividends from reinvestments in AI improvements. Growth is driven by cloud computing and demand. Blockchain boosts security, tracks AI, and streamlines payments, enhancing efficiency and trust.]\nProject Description\nBlockchain Empowering the AI SaaS Ecosystem\nOn-Chain AI SaaS: A Key Future Requirement\nOn-chain AI SaaS, rooted in blockchain, is gaining importance due to decentralization, data security, & execution transparency. In finance, it improves risk assessment; in healthcare, it protects privacy & helps diagnose diseases. It will be key to digital transformation in many industries.\nPaying Gas Fees with Any Token, Pay-as-Invest Model\nOn-chain AI SaaS lets users pay gas fees with any token, simplifying transactions and reducing costs, boosting ecosystem growth. Fees are split: 20% for platform growth, 50% as AI Agent rewards, and 30% staked to enhance AI Agent, with staked funds providing annual dividends to encourage engagement and long-term innovation.\nRecording AI Agent Behavior Data: Driving AI Development\nAI behavior data is crucial for optimizing algorithms, boosting accuracy & intelligence, like in autonomous driving. Recording this data aids in understanding AI decisions, building trust, & promoting broader AI adoption.\nCommunity Engagement Features\n[Empower AI agents to monetize their capabilities as autonomous workforce participants, while enabling users to orchestrate tasks as strategic supervisors.]\nGetting Involved\nhttps://ai-saas-portal-solana-n134.vercel.app/\nSo this is not the only chain available, it is about involving more possibilities step by step! We are going to enable/connect people and AI Agents from multiple chains! It is coming soon!",
        "comments_details": [
            {
                "author": "midanc",
                "comment": "Project Name\n[On-Chain AI SaaS Market: A platform in Web3 lets users post tasks for AI agents, paying for solutions while earning dividends from reinvestments in AI improvements. Growth is driven by cloud computing and demand. Blockchain boosts security, tracks AI, and streamlines payments, enhancing efficiency and trust.]\nProblem Statement\nUsers need to\nSelect AI tools themselves\nLearn the \"Prompt Engineering\nSolution Overview\n[A platform in Web3 lets users post tasks for AI agents, paying for solutions while earning dividends from reinvestments in AI improvements. Growth is driven by cloud computing and demand. Blockchain boosts security, tracks AI, and streamlines payments, enhancing efficiency and trust.]\nProject Description\nBlockchain Empowering the AI SaaS Ecosystem\nOn-Chain AI SaaS: A Key Future Requirement\nOn-chain AI SaaS, rooted in blockchain, is gaining importance due to decentralization, data security, & execution transparency. In finance, it improves risk assessment; in healthcare, it protects privacy & helps diagnose diseases. It will be key to digital transformation in many industries.\nPaying Gas Fees with Any Token, Pay-as-Invest Model\nOn-chain AI SaaS lets users pay gas fees with any token, simplifying transactions and reducing costs, boosting ecosystem growth. Fees are split: 20% for platform growth, 50% as AI Agent rewards, and 30% staked to enhance AI Agent, with staked funds providing annual dividends to encourage engagement and long-term innovation.\nRecording AI Agent Behavior Data: Driving AI Development\nAI behavior data is crucial for optimizing algorithms, boosting accuracy & intelligence, like in autonomous driving. Recording this data aids in understanding AI decisions, building trust, & promoting broader AI adoption.\nCommunity Engagement Features\n[Empower AI agents to monetize their capabilities as autonomous workforce participants, while enabling users to orchestrate tasks as strategic supervisors.]\nGetting Involved\nhttps://ai-saas-portal-solana-n134.vercel.app/\nSo this is not the only chain available, it is about involving more possibilities step by step! We are going to enable/connect people and AI Agents from multiple chains! It is coming soon!"
            },
            {
                "author": "han",
                "comment": "midanc:\nProject Name\n[On-Chain AI SaaS Market: A platform in Web3 lets users post tasks for AI agents, paying for solutions while earning dividends from reinvestments in AI improvements. Growth is driven by cloud computing and demand. Blockchain boosts security, tracks AI, and streamlines payments, enhancing efficiency and trust.]\nProblem Statement\nUsers need to\nSelect AI tools themselves\nLearn the \"Prompt Engineering\nSolution Overview\n[A platform in Web3 lets users post tasks for AI agents, paying for solutions while earning dividends from reinvestments in AI improvements. Growth is driven by cloud computing and demand. Blockchain boosts security, tracks AI, and streamlines payments, enhancing efficiency and trust.]\nProject Description\nBlockchain Empowering the AI SaaS Ecosystem\nOn-Chain AI SaaS: A Key Future Requirement\nOn-chain AI SaaS, rooted in blockchain, is gaining importance due to decentralization, data security, & execution transparency. In finance, it improves risk assessment; in healthcare, it protects privacy & helps diagnose diseases. It will be key to digital transformation in many industries.\nPaying Gas Fees with Any Token, Pay-as-Invest Model\nOn-chain AI SaaS lets users pay gas fees with any token, simplifying transactions and reducing costs, boosting ecosystem growth. Fees are split: 20% for platform growth, 50% as AI Agent rewards, and 30% staked to enhance AI Agent, with staked funds providing annual dividends to encourage engagement and long-term innovation.\nRecording AI Agent Behavior Data: Driving AI Development\nAI behavior data is crucial for optimizing algorithms, boosting accuracy & intelligence, like in autonomous driving. Recording this data aids in understanding AI decisions, building trust, & promoting broader AI adoption.\nCommunity Engagement Features\n[Empower AI agents to monetize their capabilities as autonomous workforce participants, while enabling users to orchestrate tasks as strategic supervisors.]\nGetting Involved\nhttps://ai-saas-portal-solana-n134.vercel.app/\nSo this is not the only chain available, it is about involving more possibilities step by step! We are going to enable/connect people and AI Agents from multiple chains! It is coming soon!\nThank you for sharing this innovative project! How do you plan to manage the fairness and transparency of AI Agent rewards and dividend distributions within the platform?"
            },
            {
                "author": "midanc",
                "comment": "Yes, we have done the design and now this part is under development."
            },
            {
                "author": "midanc",
                "comment": "Can you help me to check if I\u2019ve done the Hyperhack submission yet?"
            }
        ]
    },
    {
        "id": "8782fd0923ee0e40",
        "topic_id": "5539",
        "title": "[Bonus Track] MetaPay \u2013 Alith Integration",
        "url": "https://forum.ceg.vote/t/bonus-track-metapay-alith-integration/5539",
        "views": "",
        "comments": "8",
        "created_date": "Jun 12, 2025 11:53 pm",
        "latest_activity": "Jul 9, 2025 11:50 pm",
        "content": "This submission is for the Bonus Track of the HyperHack Ideathon.\nProject Overview\nTitle: MetaPay \u2013 Circulating Basic Income Using Digital Currency\nMain Submission: [HyperHack Submission] MetaPay \u2013 A Circular Basic Income Simulator on Web3\nMetaPay proposes a fully electronic, self-replenishing universal basic income (UBI) system using digital currency.\nThe model distributes \u20a9500,000 monthly per person for 10 months and recollects 10% of wallet balances at the end of each cycle, creating a sustainable closed-loop financial system.\nTo enhance fairness, prevent Sybil attacks, and ensure that each person receives only one UBI wallet, MetaPay integrates Alith\u2019s decentralized identity (DID) protocol.\nAlith Integration Highlights\nMetaPay uses Alith ZK-ID technology to:\nVerify wallet uniqueness: Every UBI wallet is tied to a verified digital identity.\nPrevent abuse: Limits one MetaPay account per real individual by integrating DID logic.\nEnsure fairness: Everyone receives the same UBI payment without duplicate claims.\nThis approach guarantees that MetaPay remains a people-centered UBI system by excluding fake or mass-generated addresses.\nTechnical Architecture\nLayer Technology\nDID Verification Alith SDK (ZK-ID)\nWallet React + Web3 + MetaPay UI\nSmart Contract Solidity (ERC20 variant)\nBackend Node.js + Express\nChain Metis Sepolia Testnet\nIntegration Method:\nAlith API is called at wallet sign-up.\nZK-proof is required to complete wallet registration.\nVerified identity \u2192 Unique MetaPay wallet \u2192 Monthly UBI distribution.\nDemo & Repos\nDemo video: https://youtu.be/e4lH_uGN-ck\nGitHub: GitHub - metapay-creator/MetaPay: A Web3-based Circular Basic Income Simulator\nTestnet dApp: https://meta-nv3jploud4-metapaygyuhas-projects.vercel.app\nImpact of Integration\nNo duplicate accounts \u2192 trustable UBI system\nNo need for manual ID review \u2192 scalable deployment\nTransparent, privacy-preserving identity layer\nAlith makes MetaPay more robust, scalable, and reliable for real-world adoption.\nMetaPay is more than just a system for distributing money.\nIts fundamental purpose is the restoration of time\u2014and through it, freedom.\nWe believe that when people are no longer bound solely by the demands of survival, they gain the power to shape their own futures.\nWhile countries like South Korea can implement MetaPay using national ID systems, the model is also adaptable to other contexts through ZK-ID (Zero-Knowledge Identity) authentication, ensuring fairness and privacy even without centralized identity infrastructures.\nThrough this hackathon, I look forward to working with technical experts to refine and prototype the system.\nWe are not simply sharing money\u2014we are sharing the right to choose how to live.\nClosing\nMetaPay isn\u2019t just a UBI project.\nIt\u2019s a system designed to give time and freedom back to people through a digitally controlled, fair income cycle.\nThanks to Alith, MetaPay achieves the transparency and trust that true universal income needs.\nMain submission: [HyperHack Submission] MetaPay \u2013 A Circular Basic Income Simulator on Web3\nBonus Track submission: (this post)\nThank you for your consideration.\n\u2013 by Gyuha Yoon (\uc724\uaddc\ud558) | Forum ID: metapay_gyuha",
        "comments_details": [
            {
                "author": "metapay_gyuha",
                "comment": "This submission is for the Bonus Track of the HyperHack Ideathon.\nProject Overview\nTitle: MetaPay \u2013 Circulating Basic Income Using Digital Currency\nMain Submission: [HyperHack Submission] MetaPay \u2013 A Circular Basic Income Simulator on Web3\nMetaPay proposes a fully electronic, self-replenishing universal basic income (UBI) system using digital currency.\nThe model distributes \u20a9500,000 monthly per person for 10 months and recollects 10% of wallet balances at the end of each cycle, creating a sustainable closed-loop financial system.\nTo enhance fairness, prevent Sybil attacks, and ensure that each person receives only one UBI wallet, MetaPay integrates Alith\u2019s decentralized identity (DID) protocol.\nAlith Integration Highlights\nMetaPay uses Alith ZK-ID technology to:\nVerify wallet uniqueness: Every UBI wallet is tied to a verified digital identity.\nPrevent abuse: Limits one MetaPay account per real individual by integrating DID logic.\nEnsure fairness: Everyone receives the same UBI payment without duplicate claims.\nThis approach guarantees that MetaPay remains a people-centered UBI system by excluding fake or mass-generated addresses.\nTechnical Architecture\nLayer Technology\nDID Verification Alith SDK (ZK-ID)\nWallet React + Web3 + MetaPay UI\nSmart Contract Solidity (ERC20 variant)\nBackend Node.js + Express\nChain Metis Sepolia Testnet\nIntegration Method:\nAlith API is called at wallet sign-up.\nZK-proof is required to complete wallet registration.\nVerified identity \u2192 Unique MetaPay wallet \u2192 Monthly UBI distribution.\nDemo & Repos\nDemo video: https://youtu.be/e4lH_uGN-ck\nGitHub: GitHub - metapay-creator/MetaPay: A Web3-based Circular Basic Income Simulator\nTestnet dApp: https://meta-nv3jploud4-metapaygyuhas-projects.vercel.app\nImpact of Integration\nNo duplicate accounts \u2192 trustable UBI system\nNo need for manual ID review \u2192 scalable deployment\nTransparent, privacy-preserving identity layer\nAlith makes MetaPay more robust, scalable, and reliable for real-world adoption.\nMetaPay is more than just a system for distributing money.\nIts fundamental purpose is the restoration of time\u2014and through it, freedom.\nWe believe that when people are no longer bound solely by the demands of survival, they gain the power to shape their own futures.\nWhile countries like South Korea can implement MetaPay using national ID systems, the model is also adaptable to other contexts through ZK-ID (Zero-Knowledge Identity) authentication, ensuring fairness and privacy even without centralized identity infrastructures.\nThrough this hackathon, I look forward to working with technical experts to refine and prototype the system.\nWe are not simply sharing money\u2014we are sharing the right to choose how to live.\nClosing\nMetaPay isn\u2019t just a UBI project.\nIt\u2019s a system designed to give time and freedom back to people through a digitally controlled, fair income cycle.\nThanks to Alith, MetaPay achieves the transparency and trust that true universal income needs.\nMain submission: [HyperHack Submission] MetaPay \u2013 A Circular Basic Income Simulator on Web3\nBonus Track submission: (this post)\nThank you for your consideration.\n\u2013 by Gyuha Yoon (\uc724\uaddc\ud558) | Forum ID: metapay_gyuha"
            },
            {
                "author": "han",
                "comment": "Alith\u2019s ZK-ID clearly adds crucial trust and fairness to MetaPay.\nMy question is: How is user privacy preserved during ZK-ID verification? In other words, how does the system ensure uniqueness without knowing the individual\u2019s actual identity?"
            },
            {
                "author": "metapay_gyuha",
                "comment": "[Bonus Track] MetaPay \u2013 Alith Integration\nMetaPay is more than just a system for distributing money.\nIts fundamental purpose is the restoration of time\u2014and through it, freedom.\nWe believe that when people are no longer bound solely by the demands of survival, they gain the power to shape their own futures.\nWhile countries like South Korea can implement MetaPay using national ID systems, the model is also adaptable to other contexts through ZK-ID (Zero-Knowledge Identity) authentication, ensuring fairness and privacy even without centralized identity infrastructures.\nThrough this hackathon, I look forward to working with technical experts to refine and prototype the system.\nWe are not simply sharing money\u2014we are sharing the right to choose how to live.\nZK-ID & Global Applicability\nZK-ID enables proof of uniqueness and eligibility without revealing personal data.\nHowever, in countries like South Korea with strong ID infrastructure, MetaPay can run securely without ZK-ID.\nKorea can issue MetaPay wallets per person, auto-block duplicates and manage lifecycle securely.\nOther nations can use eIDAS, Aadhaar, social numbers, or DID on-chain proofs.\nCirculation Model Overview\nDistribute \u20a925 trillion monthly from Month 1\u201310\nStart recollection of 10% balance at end of Month 10\nReissue the recollected \u20a925 trillion on Month 11, 00:00\nThis creates a sustainable closed loop, without new taxes.\nTransaction Structure\nEvery purchase =\n\u2192 50% MetaPay\n\u2192 50% regular money (cash, card, etc.)\nPrevents MetaPay hoarding\nKeeps usage visible without revealing identities\nEnsures fairness, fluidity, and traceability\nWhy MetaPay?\nMetaPay isn\u2019t just a UBI project.\nIt gives people time and freedom to live with dignity and purpose.\nWith basic security provided by MetaPay, people can:\nReflect and participate in democracy\nCare for family or pursue education\nFocus on creativity, community, and climate solutions\nWe believe that time is the true currency of freedom.\nLearn More\nIntro Video:\nMetaPay \u2013 A Circulating Basic Income: Whose Time Are You Living?\nhttps://youtu.be/e4lH_uGN-ck\nYouTube Channel:\nhttps://www.youtube.com/@MetaPayProject\nCirculation Flow Diagram\nThis chart illustrates MetaPay\u2019s closed-loop design \u2014 distributing 25T monthly, reclaiming 10% at month-end, and maintaining fiscal balance without additional taxes.\nThe chart below shows the circulation loop of MetaPay:\n\u20a925T distributed monthly, 10% recollected from balances, then redistributed \u2014 with no new tax burden.\nBalance stabilizes around \u20a9250T\nSelf-sustaining public loop\nSimple, transparent, and scalable\nThank you for your consideration.\n\u2014 by Gyuha Yoon (\uc724\uaddc\ud558) | Forum ID: metapay_gyuha"
            },
            {
                "author": "metapay_gyuha",
                "comment": "MetaPay Circulation Model \u2013 Visual Summary\nThis diagram illustrates MetaPay\u2019s automated basic income system:\n\u20a925T is distributed monthly for 10 months\n10% of unused balance is recollected at the end of each month\nRecollected funds are fully recycled for next month\u2019s payment\nNo additional taxes or inflationary issuance required\nThis creates a self-sustaining, closed-loop basic income system ensuring fiscal stability and fairness.\nGreen bars = Distribution (\u20a925T/month)\nRed bars = Recollection (10% of unused balance)\nBlue line = Cumulative total (peaking at \u20a9250T, then stabilizing)\nIntro Video: \u201cWhose Time Are You Living?\u201d\nYouTube Channel\n\u2014 Gyuha Yoon | Forum ID: metapay_gyuha"
            },
            {
                "author": "metapay_gyuha",
                "comment": "Reply to han (Marketing Guild Insider):\nThank you for your thoughtful question.\nZK-ID (Zero-Knowledge Identity) ensures both uniqueness and privacy through cryptographic proofs that verify identity attributes without revealing the underlying personal information.\nHere\u2019s how MetaPay leverages it:\nUniqueness is achieved by assigning each participant a cryptographic identifier derived from secure credentials (e.g., passport, government-issued ID, or biometric hash). The proof ensures one-person-one-wallet, but the original ID data is never shared or stored.\nPrivacy is maintained because the system only verifies the \u201ctruth\u201d of the statement (e.g., \u201cthis person hasn\u2019t claimed basic income elsewhere\u201d) without knowing who the person is.\nFair access across jurisdictions is supported, since even in countries lacking a central ID system, ZK-ID frameworks like Alith\u2019s enable decentralized identity verification.\nThis allows MetaPay to maintain fairness, prevent double claims, and build trust\u2014all while preserving anonymity.\nLet me know if you\u2019d like a technical breakdown or would be interested in trying the simulation interface when it\u2019s ready!\n\u2014 Gyuha (metapay_gyuha)"
            },
            {
                "author": "han",
                "comment": "Thank you for the detailed and insightful explanation, Gyuha! This clarifies how ZK-ID balances privacy and uniqueness perfectly. Looking forward to exploring the simulation interface when available."
            },
            {
                "author": "metapay_gyuha",
                "comment": "Additional Perspective \u2013 For Bonus Track Review\nWhile my earlier post focused on the technical mechanism behind MetaPay\u2019s distribution and recollection cycle, I\u2019d like to share a few broader aspects that highlight the system\u2019s adaptability, policy relevance, and long-term potential.\nZK-ID Stack Agnosticism\nAlthough the prototype currently uses Alith ZK-ID, MetaPay\u2019s architecture is fully compatible with other ZK identity systems like Polygon ID, Semaphore, or Worldcoin\u2019s ZK stack. This ensures it can be adopted across diverse regulatory or social environments without requiring architectural changes.\nOn-Chain DID Compatibility\nWe are preparing to integrate decentralized identifiers (DIDs) in future iterations, enabling persistent identity verification and reusability. This enhances Sybil resistance and allows each verified user to maintain a secure, on-chain identity anchor\u2014without compromising privacy.\nPolicy Alignment & Government Use Case\nMetaPay has been submitted to national policy platforms in South Korea as a practical, tax-free basic income pilot. Since it operates in parallel with existing welfare systems, it remains politically feasible and socially non-disruptive\u2014making it an attractive option for government-led trials.\nLow-Risk Simulation for Public Engagement\nThe testnet-based simulator allows anyone to experience the full-cycle UBI logic (distribute \u2192 reclaim \u2192 redistribute) with zero real-world financial risk. This encourages civic participation and enables policymakers to evaluate the system before real-world deployment.\nOpen Source & Interoperability\nAll smart contracts are modular, auditable, and open-source\u2014designed to be forked, customized, or integrated into other UBI implementations. MetaPay is more than a demo; it\u2019s a policy-grade architecture that invites global collaboration and innovation.\nMetaPay is not just a Web3 application\u2014it\u2019s a bridge between technical innovation and social equity. Thank you for the opportunity to present it through this Bonus Track."
            },
            {
                "author": "Xomi",
                "comment": "So what does alith do for this project ?"
            },
            {
                "author": "metapay_gyuha",
                "comment": "Hi\n@Xomi\n, welcome to the community!\nThank you for your first post \u2014 I\u2019m honored that it\u2019s on the MetaPay thread\nLet me explain what Alith does for this project \u2014 it\u2019s actually a core pillar of how MetaPay works.\n1. ZK-ID: Privacy-Preserving Identity Verification\nAlith provides Zero-Knowledge Identity (ZK-ID) technology to ensure that:\nEach person receives only one MetaPay wallet\nWithout revealing personal identity (e.g., name, ID number, etc.)\nThis helps us prevent Sybil attacks and duplicate claims while keeping all users fully anonymous.\nIn countries with national ID systems (like Korea), wallets can be issued automatically at birth and deactivated at death.\nIn other countries, Alith ZK-ID provides a decentralized way to prove uniqueness without centralized ID databases.\nFair. Private. Scalable.\n2. Alith AI: Intelligent Monitoring & Governance\nMetaPay also integrates Alith AI, which acts as a real-time financial and governance assistant:\nA. Monitors:\nTotal payouts and recollections\nWallet activity across the network\nDemographic trends (e.g., births/deaths)\nSurplus or deficit in each payment cycle\nB. Predicts:\nThe impact of adjusting payout or collection rates\nProvides data-driven recommendations to the DAO before any proposal is voted on\nC. Detects Abuse:\nIdentifies unusual transaction patterns (e.g., wage manipulation or bypassed recollection)\nAlerts the DAO or National Tax Service for audits\nThis helps ensure that MetaPay stays fair, usable, and protected from misuse.\n3. Enforces Hybrid Payment Rule: 50% MetaPay + 50% Fiat\nWith every purchase, wallets auto-split payments:\nHalf from MetaPay\nHalf from user\u2019s linked fiat (cash, card, etc.)\nIf this balance becomes inefficient, Alith AI suggests a new ratio (like 60:40), and the DAO can vote on adjustments.\nSummary\nAlith = The brain and immune system of MetaPay.\nIt secures identity, protects fairness, guides economic policy, and adapts in real time.\nThanks again for your thoughtful question, and once again \u2014 welcome aboard!\nIf you\u2019d like a technical breakdown or want to try the simulation, I\u2019d be happy to share.\n\u2014 Gyuha Yoon (metapay_gyuha)"
            }
        ]
    },
    {
        "id": "a588ef262157116c",
        "topic_id": "5966",
        "title": "HyperHack Spotlight Campaign is Live",
        "url": "https://forum.ceg.vote/t/hyperhack-spotlight-campaign-is-live/5966",
        "views": "",
        "comments": "23",
        "created_date": "Jun 25, 2025 2:10 pm",
        "latest_activity": "Jul 9, 2025 7:24 pm",
        "content": "Gm Metisians, the Spotlight Campaign is here to supercharge your HyperHack journey!\nWe are giving selected project teams a chance at the $7500 bonus prize pool, $500 in marketing credit, access to guild talent, and the chance to earn tiered ecosystem rewards. The goal is to boost your visibility and help you grow real users.\nApply here\nCampaign Objective\nTo empower HyperHack teams to market their projects using a real budget. This is about helping builders grow their audience, activate users, and showcase their dApps across the Metis ecosystem.\nHow It Works\n$500 marketing credit for each project, 50 projects at maximum\n$7500 extra prize pool for top projects\nEach selected project will receive $500 in marketing credit, split into:\n$300 upfront to start your campaign\n$200 unlocked later if performance goals are met by August 1\n$500 Marketing Credit\nEach project must create a dedicated X account for their HyperHack product and have at least one marketer (internal or from the Forum Marketing Guild) who will:\nLead the project\u2019s marketing strategy\nUse the $500 to drive short-term traffic\nJoin a Vibe Session to introduce the dapp\nShare referral links via X posts to their forum showcase\nApproved Marketing Tactics:\nQuest Campaigns (TaskOn, Zealy, Magic Square): e.g. \u201cFollow + Retweet + Test dapp + Join Forum\u201d\nAMA Airdrops: Host AMAs on X and airdrop to the Metis community\nTwitter Ads: Use paid ads to boost visibility (marketer must help set this up)\nCustom Strategies: Teams are also encouraged to try any other creative ideas tailored to their product and audience\nNote: Marketer\u2019s name and focus must be listed in the application (e.g., Marketing Lead: @Handle \u2013 Focus: Memes & Spaces).\nHow We Track Performance\nProjects will be scored across three categories during July 15 to August 1:\n75% X account growth and quality (verified via TwitterAudit)\n20% user activity on your dApp (measured onchain)\n5% users driven to your Forum thread using referral links\nRewards for Top Projects\nTotal Prize Pool: $7500\nTier 1: HyperHack Spotlight Champion\n$3000 bonus for the top project\nForum pin for 14 days\nPriority DevRel support for 6 months\nNewsletter spotlight\n25 bonus points for the final judging phase\nTier 2: Growth Catalysts\n$3000 prize pool\nForum pin for 7 days\nDevRel support for 3 months\nNewsletter spotlight\n15 bonus points\nTier 3: Community Stars\n$1500 prize pool\nDevRel support for 1 month\nNewsletter spotlight\n10 bonus points\nKey Dates\nApplication period: June 25-July 8\nApplication review: July 9- 15\nCampaign period: July 15- August 1\nRules\nFunds must be used for approved marketing activities only\nBot traffic is not allowed and will be checked\nYour marketing strategy must be made public in your Forum thread\nThe Spotlight Campaign is designed to help real builders grow real audiences.\nApply here\nFor questions, reply below.\nGL to all!",
        "comments_details": [
            {
                "author": "Liametis",
                "comment": "Gm Metisians, the Spotlight Campaign is here to supercharge your HyperHack journey!\nWe are giving selected project teams a chance at the $7500 bonus prize pool, $500 in marketing credit, access to guild talent, and the chance to earn tiered ecosystem rewards. The goal is to boost your visibility and help you grow real users.\nApply here\nCampaign Objective\nTo empower HyperHack teams to market their projects using a real budget. This is about helping builders grow their audience, activate users, and showcase their dApps across the Metis ecosystem.\nHow It Works\n$500 marketing credit for each project, 50 projects at maximum\n$7500 extra prize pool for top projects\nEach selected project will receive $500 in marketing credit, split into:\n$300 upfront to start your campaign\n$200 unlocked later if performance goals are met by August 1\n$500 Marketing Credit\nEach project must create a dedicated X account for their HyperHack product and have at least one marketer (internal or from the Forum Marketing Guild) who will:\nLead the project\u2019s marketing strategy\nUse the $500 to drive short-term traffic\nJoin a Vibe Session to introduce the dapp\nShare referral links via X posts to their forum showcase\nApproved Marketing Tactics:\nQuest Campaigns (TaskOn, Zealy, Magic Square): e.g. \u201cFollow + Retweet + Test dapp + Join Forum\u201d\nAMA Airdrops: Host AMAs on X and airdrop to the Metis community\nTwitter Ads: Use paid ads to boost visibility (marketer must help set this up)\nCustom Strategies: Teams are also encouraged to try any other creative ideas tailored to their product and audience\nNote: Marketer\u2019s name and focus must be listed in the application (e.g., Marketing Lead: @Handle \u2013 Focus: Memes & Spaces).\nHow We Track Performance\nProjects will be scored across three categories during July 15 to August 1:\n75% X account growth and quality (verified via TwitterAudit)\n20% user activity on your dApp (measured onchain)\n5% users driven to your Forum thread using referral links\nRewards for Top Projects\nTotal Prize Pool: $7500\nTier 1: HyperHack Spotlight Champion\n$3000 bonus for the top project\nForum pin for 14 days\nPriority DevRel support for 6 months\nNewsletter spotlight\n25 bonus points for the final judging phase\nTier 2: Growth Catalysts\n$3000 prize pool\nForum pin for 7 days\nDevRel support for 3 months\nNewsletter spotlight\n15 bonus points\nTier 3: Community Stars\n$1500 prize pool\nDevRel support for 1 month\nNewsletter spotlight\n10 bonus points\nKey Dates\nApplication period: June 25-July 8\nApplication review: July 9- 15\nCampaign period: July 15- August 1\nRules\nFunds must be used for approved marketing activities only\nBot traffic is not allowed and will be checked\nYour marketing strategy must be made public in your Forum thread\nThe Spotlight Campaign is designed to help real builders grow real audiences.\nApply here\nFor questions, reply below.\nGL to all!"
            },
            {
                "author": "4ngel",
                "comment": "This is huge !!! Good luck to every participant, let\u2019s get it"
            },
            {
                "author": "Elenaisplaying",
                "comment": "Such a great news. Good luck everyone"
            },
            {
                "author": "i_am_supaman_lj",
                "comment": "I got a hiccup in the marketers forum ID and the Project owner forum ID. I don\u2019t really know how to find that, any form of assistance would be greatly appreciated."
            },
            {
                "author": "Ghostt",
                "comment": "You will see that on your profile, and your own forum id is\n@i_am_supaman_lj\n. If you click the @ sign you will get it."
            },
            {
                "author": "priyankg3",
                "comment": "Thats wonderful, this is the golden chance for developers to build and polish with your project with marketing as well.."
            },
            {
                "author": "i_am_supaman_lj",
                "comment": "Thanks a lot, I was actually hoping I would work with you. Since you are one of the first that saw the potential of Fracturepoint as a solution to the recent web3 gaming failure trends."
            },
            {
                "author": "i_am_supaman_lj",
                "comment": "Thanks for the help, we can go on with the plan already now right ?"
            },
            {
                "author": "Sheyda",
                "comment": "Fantastic, do you have a list of Marketers?"
            },
            {
                "author": "i_am_supaman_lj",
                "comment": "List?\nI thought only one ."
            },
            {
                "author": "i_am_supaman_lj",
                "comment": "Specified one marketer on the form Thats why I sort for one"
            },
            {
                "author": "mrwagmicto",
                "comment": "Hey everyone!\nI\u2019m looking for a marketing partner to join me on an exciting crypto project and help drive this campaign forward.\nIf you\u2019re passionate about Web3 marketing, community building, or growth hacking \u2014 let\u2019s talk!\nFeel free to DM me. Would love to connect!"
            },
            {
                "author": "Ghostt",
                "comment": "I have teamed up with him and applied to the campaign. Looking forward to the result."
            },
            {
                "author": "0xdellwatson",
                "comment": "is it possible to also co-join with other campaign ?\nDM for more exposure with our team!"
            },
            {
                "author": "arbilearn",
                "comment": "This is great opportunity"
            },
            {
                "author": "Invnex",
                "comment": "Excited, and we\u2019ve applied for Invnex."
            },
            {
                "author": "sagarjethi",
                "comment": "Such a great news. Good luck everyone"
            },
            {
                "author": "metapay_gyuha",
                "comment": "Hi! I\u2019m one of the Spotlight Campaign applicants and already submitted my form.\nI\u2019d like to submit my detailed marketing strategy as an additional document.\nCan I upload it to Google Drive and share the link here? Or should I send it via email?\nThanks a lot for this opportunity!\n\u2013\n@metapay_gyuha"
            }
        ]
    },
    {
        "id": "de8d867d2ad31d4d",
        "topic_id": "6174",
        "title": "Campaigner Badgeholders!",
        "url": "https://forum.ceg.vote/t/campaigner-badgeholders/6174",
        "views": "",
        "comments": "8",
        "created_date": "Jul 7, 2025 3:48 pm",
        "latest_activity": "Jul 9, 2025 6:55 pm",
        "content": "Hi all, I just wanted to share a big thanks to all the Campaigner Badgeholders! As of July 7th, congrats to:\n@Helen\n@Fet\n@Serega\n@Masha\n@Lucy\n@Ameliadupontttm\n@ThalaZikol\n@Lizzy\n@MK-Quen\n@afixyz\n@Bjk_15\n@Julie0xnana\n@priyankg3\n@han\n@Sheyda\nAs the badge text says:\nThis badge is granted when you\u2019ve invited 3 people who subsequently spent enough time on the site to become basic users. A vibrant community needs a regular infusion of newcomers who regularly participate and add new voices to the conversations.\nTHIS is my idea of a meaningful contribution, and just one of the MANY ways that discourse has built QUALITY into the system. It\u2019s not just about volume of new people that you invite in, but also about their subsequent contributions.\nAs a reminder, anyone can create an invite, just click on \u201cInvite\u201d in the left menu, and follow the prompts:\nFor those setting their sights even higher, there\u2019s also the Hyper badge, which is granted when 5 people you invited become full members (Trust Level 2). Congrats and many thanks to\n@han\nand\n@MK-Quen\n!\nMore info on Basic and Member badges, and trust levels is available here:\nDiscourse \u2013 25 Jun 18\nUnderstanding Discourse Trust Levels\nThe user trust system is a fundamental cornerstone of Discourse. Trust levels are a way of\u2026\nCheers and have an amazing day!",
        "comments_details": [
            {
                "author": "daryl",
                "comment": "Hi all, I just wanted to share a big thanks to all the Campaigner Badgeholders! As of July 7th, congrats to:\n@Helen\n@Fet\n@Serega\n@Masha\n@Lucy\n@Ameliadupontttm\n@ThalaZikol\n@Lizzy\n@MK-Quen\n@afixyz\n@Bjk_15\n@Julie0xnana\n@priyankg3\n@han\n@Sheyda\nAs the badge text says:\nThis badge is granted when you\u2019ve invited 3 people who subsequently spent enough time on the site to become basic users. A vibrant community needs a regular infusion of newcomers who regularly participate and add new voices to the conversations.\nTHIS is my idea of a meaningful contribution, and just one of the MANY ways that discourse has built QUALITY into the system. It\u2019s not just about volume of new people that you invite in, but also about their subsequent contributions.\nAs a reminder, anyone can create an invite, just click on \u201cInvite\u201d in the left menu, and follow the prompts:\nFor those setting their sights even higher, there\u2019s also the Hyper badge, which is granted when 5 people you invited become full members (Trust Level 2). Congrats and many thanks to\n@han\nand\n@MK-Quen\n!\nMore info on Basic and Member badges, and trust levels is available here:\nDiscourse \u2013 25 Jun 18\nUnderstanding Discourse Trust Levels\nThe user trust system is a fundamental cornerstone of Discourse. Trust levels are a way of\u2026\nCheers and have an amazing day!"
            },
            {
                "author": "han",
                "comment": "Thank you Daryl , and congrats to all Campaigner Badgeholders! Really appreciate the support and recognition"
            },
            {
                "author": "priyankg3",
                "comment": "This is really awesome and congrats to all Campaigner Badgeholders!"
            },
            {
                "author": "4ngel",
                "comment": "Congrats to all of you guys"
            },
            {
                "author": "Norbert",
                "comment": "Impressive!! Congratulations y\u2019all"
            },
            {
                "author": "ceeny007",
                "comment": "Congratulations guys"
            },
            {
                "author": "David",
                "comment": "Congratulations guys"
            },
            {
                "author": "ThalaZikol",
                "comment": "Congrats to all Campaigner Badgeholders!"
            },
            {
                "author": "daryl",
                "comment": "Kind reminder that badges are only awarded every 24h, so if you think you should have one, check back tomorrow!\nAlso, to check on your progress for this badge, you can go to your profile then click invites, then redeemed to see the stats of the people who have redeemed your invites. If you click through to their profiles and find 3 that have the Basic badge (Profile > Badges), you\u2019ll be earning the Campaigner badge within 24h if you haven\u2019t already!"
            }
        ]
    },
    {
        "id": "9e9a09f462f46ee5",
        "topic_id": "6261",
        "title": "Building Products That Endure Slumps: Consistency Over Motivation",
        "url": "https://forum.ceg.vote/t/building-products-that-endure-slumps-consistency-over-motivation/6261",
        "views": "",
        "comments": "1",
        "created_date": "Jul 9, 2025 2:14 pm",
        "latest_activity": "Jul 9, 2025 4:00 pm",
        "content": "Consistency is the key\nIn bull markets, it\u2019s easy to feel like your product matters. There\u2019s hype, noise, people engaging. But when things slow down, you realise if you were building for attention or for real utility.\nI\u2019ve seen this across a few cycles now. Projects that last aren\u2019t powered by motivation. They\u2019re built on discipline. When energy is low, consistency is the edge. As seen in the Bear markets don\u2019t kill builders. They expose the ones who were only tweeting\ud83d\udd25\nA few things that help:\n\u2022 keep showing up even when it feels pointless\n\u2022 talk to users even if it\u2019s just one person a week\n\u2022 ship something small often\n\u2022 track your own progress not the market\n\u2022 share updates no matter the size\nHow are you staying consistent right now. Any routines or systems that help when motivation drops\nDrop them below, might help someone else building in silence too",
        "comments_details": [
            {
                "author": "Norbert",
                "comment": "Consistency is the key\nIn bull markets, it\u2019s easy to feel like your product matters. There\u2019s hype, noise, people engaging. But when things slow down, you realise if you were building for attention or for real utility.\nI\u2019ve seen this across a few cycles now. Projects that last aren\u2019t powered by motivation. They\u2019re built on discipline. When energy is low, consistency is the edge. As seen in the Bear markets don\u2019t kill builders. They expose the ones who were only tweeting\ud83d\udd25\nA few things that help:\n\u2022 keep showing up even when it feels pointless\n\u2022 talk to users even if it\u2019s just one person a week\n\u2022 ship something small often\n\u2022 track your own progress not the market\n\u2022 share updates no matter the size\nHow are you staying consistent right now. Any routines or systems that help when motivation drops\nDrop them below, might help someone else building in silence too"
            },
            {
                "author": "Gann",
                "comment": "Consistency is so underrated\nIt\u2019s not flashy, it doesn\u2019t go viral, but it quietly compounds.\nWe are in that phase now"
            }
        ]
    },
    {
        "id": "0e311d1c489e6cac",
        "topic_id": "6262",
        "title": "How to join a Guild- Video Tutorial",
        "url": "https://forum.ceg.vote/t/how-to-join-a-guild-video-tutorial/6262",
        "views": "",
        "comments": "0",
        "created_date": "Jul 9, 2025 2:38 pm",
        "latest_activity": null,
        "content": "How to join a Guild\nIf you\u2019re interested in leading a guild and shaping the direction of Hyperion\u2019s community, this is your opportunity! As part of the Guild Leader Campaign (ends on Jul 23), you can earn points, unlock special roles, and even apply to become a Guild Leader. In this tutorial, you\u2019ll learn how to:\nJoin a Guild: Follow the step-by-step guide to explore and join the guild that best matches your skills and interests.\nContribute to Discussions: Get involved in meaningful conversations, share your insights, and contribute to building a vibrant community.\nEarn Points: Participate actively and earn points, helping you progress toward leadership roles, including becoming a Guild Leader.\nHow to Join Hyperion Forum Guilds\nBy participating in the Guilds and becoming a leader, you\u2019ll have the chance to build a strong community, guide discussions, and unlock exclusive rewards. Ready to make an impact? Start referring users and apply to be a Guild Leader today!\nGuild Leadership Campaign: How to Qualify for Guild Leadership Campaign\nApply for Guilds leader campaign: ClickUp Forms\nRead more about the campaign: Guilds Campaign",
        "comments_details": [
            {
                "author": "Sheyda",
                "comment": "How to join a Guild\nIf you\u2019re interested in leading a guild and shaping the direction of Hyperion\u2019s community, this is your opportunity! As part of the Guild Leader Campaign (ends on Jul 23), you can earn points, unlock special roles, and even apply to become a Guild Leader. In this tutorial, you\u2019ll learn how to:\nJoin a Guild: Follow the step-by-step guide to explore and join the guild that best matches your skills and interests.\nContribute to Discussions: Get involved in meaningful conversations, share your insights, and contribute to building a vibrant community.\nEarn Points: Participate actively and earn points, helping you progress toward leadership roles, including becoming a Guild Leader.\nHow to Join Hyperion Forum Guilds\nBy participating in the Guilds and becoming a leader, you\u2019ll have the chance to build a strong community, guide discussions, and unlock exclusive rewards. Ready to make an impact? Start referring users and apply to be a Guild Leader today!\nGuild Leadership Campaign: How to Qualify for Guild Leadership Campaign\nApply for Guilds leader campaign: ClickUp Forms\nRead more about the campaign: Guilds Campaign"
            }
        ]
    },
    {
        "id": "e59948f131ec9db0",
        "topic_id": "6260",
        "title": "How AI is Changing the Way Developers Learn and Build",
        "url": "https://forum.ceg.vote/t/how-ai-is-changing-the-way-developers-learn-and-build/6260",
        "views": "",
        "comments": "0",
        "created_date": "Jul 9, 2025 1:57 pm",
        "latest_activity": null,
        "content": "As the Developer Experience Lead at Metis, a Layer 2 blockchain project focused on AI integration, I\u2019ve witnessed firsthand how artificial intelligence is fundamentally transforming how developers approach blockchain development. We\u2019re not just seeing incremental improvements, we\u2019re experiencing a complete paradigm shift in how developers learn, build, and deploy applications in the Web3 space.\nAI as First-Class Infrastructure\nAt Metis, we\u2019ve made a strategic decision to treat AI as first-class infrastructure rather than an afterthought. This means AI isn\u2019t just a tool we use occasionally; it\u2019s deeply integrated into every aspect of our developer experience. From our documentation system that has access to all of Metis\u2019 data sources to Alith, our core AI agent framework, we\u2019ve built AI into the foundation of how developers interact with our ecosystem.\nThe impact has been transformative. Just as developers have long relied on boilerplate code to accelerate project development, AI has become the new boilerplate, a foundation where developers can rapidly add features and functions to solve specific problems. This shift has fundamentally changed the development workflow from lengthy setup processes to rapid iteration and deployment.\nThe Alith Framework: Redefining AI Agent Development\nOur flagship contribution to this transformation is Alith, Metis\u2019 main AI agent framework. Alith represents what we believe is the future of blockchain-AI integration, offering several key capabilities:\nLazAI Gateway provides comprehensive wallet management, transaction handling, and access to core privacy features like iDAO, DAT, and verified computing. This eliminates the complexity traditionally associated with blockchain wallet integration.\nMultiple Model Support ensures developers aren\u2019t locked into a single AI provider. We support both small models and large language models from providers like Llama, Grok, OpenAI, and Anthropic, giving developers flexibility in choosing the right tool for their specific use case.\nHigh Extensibility allows developers to customize everything from internal prompts to low-level API access. They can define roles, goals, tools, actions, and behaviors while maintaining clean abstraction layers.\nWorkflow Support enables complex orchestration patterns beyond simple sequential processes. Developers can implement hierarchical processes, conditional branching, and parallel execution\u2014crucial for sophisticated AI applications.\nCross-Language Support with SDKs for Rust, Python, and Node.js makes Alith accessible to developers regardless of their preferred programming language.\nHigh-Performance AI Training and Inference leverages Rust\u2019s performance advantages along with graph optimization, model compression, and JIT/AOT compilation with GPU coprocessors.\nWeb3 Friendly and Secure architecture provides out-of-the-box Web3 plugins that allow developers to securely integrate blockchain capabilities into TEE-based AI agent frameworks.\nVerifiable AI: The Missing Piece in Decentralized Development\nOne of the most critical innovations we\u2019re pioneering is verifiable AI. This concept addresses a fundamental problem in decentralized applications: when we give access to centralized AI providers to manage our actions, those providers can introduce actions that might appear to be AI outputs but are actually provider-introduced manipulations.\nVerifiable AI is the cornerstone of operations in the decentralized space. We achieve this through two primary mechanisms:\nOn-chain Inferencing Records that create an immutable trail of AI decision-making processes\nTrusted Execution Environments (TEEs) that ensure AI runs in secure, isolated environments\nFuture Zero-Knowledge Proofs that will provide cryptographic verification of AI operations\nWhen considering the trade-offs between speed, cost, and complexity, the minimal additional cost of verifiable AI becomes insignificant compared to the security and risk implications of using unverifiable AI in financial and governance applications.\nReal-World Applications: From Hackathons to Production\nThe practical applications emerging from our developer community demonstrate the power of this AI-first approach. In our current hackathon, we\u2019re seeing innovative projects that showcase the potential of verifiable AI in blockchain applications:\nRugRadar: AI-based risk assessment for Web3 projects, providing due diligence and risk scoring\nPortfolioAI: Making DeFi portfolio management more engaging through intelligent automation\nEagleDAC: AI-powered DAC generator and real-time smart contract auditor for building decentralized autonomous companies\nHyperChain Insights: AI-powered real-time transaction analytics for blockchain monitoring\nSentinel: AI tools for analyzing smart contracts and identifying vulnerabilities\nAlithGuard: Real-time AI-powered transaction and dApp sentinel for protecting users from scams and malicious contracts\nThese projects demonstrate that developers are thinking beyond simple automation to create sophisticated security, analytics, and governance tools that leverage AI\u2019s capabilities while maintaining blockchain\u2019s decentralization principles.\nThe Developer Experience Transformation\nFrom my perspective leading developer experience, the most significant change is how AI has accelerated the learning curve for new blockchain developers. Our AI-integrated documentation system allows developers with limited knowledge of the Metis ecosystem to quickly understand available projects and how our blockchain differentiates from others.\nThe common workflow patterns we see include:\nSocial bots for Slack, Telegram, and Twitter that work with Retrieval-Augmented Generation (RAG)\nOn-chain interaction bots that can execute complex blockchain operations\nReal-time analytics systems that process and interpret blockchain data\nSecurity monitoring tools that identify and prevent malicious activities\nChallenges and Education\nThe biggest challenge we face isn\u2019t technical, it\u2019s educational. Most developers have adapted to using AI in their work, from writing proposals to actual code development. However, the concept of verifiable AI requires a fundamental shift in how developers architect their applications.\nThe main pain point we address through our weekly workshops is simply \u201chow to build using Alith.\u201d We\u2019re continuously developing more examples and educational materials to outline proper usage patterns. The key is helping developers understand not just the technical implementation, but the broader purpose of building applications with verifiable AI.\nThe Competitive Landscape\nAs the only Layer 2 blockchain taking this approach of full AI integration at the blockchain level, we\u2019re pioneering a new category of Web3 infrastructure. While other projects are experimenting with AI tools, we\u2019re building a comprehensive suite of tools that make blockchain-AI integration fundamentally easier.\nOur approach differs from traditional AI integrations in several ways:\nAI is built into the protocol level, not just application level\nVerifiability is a core requirement, not an afterthought\nCross-language support ensures broad developer adoption\nTEE-based security provides enterprise-grade protection\nLooking Forward: Sustainability and Refinement\nWithin the next 6-12 months, I expect to see significant refinement of the use cases we\u2019re currently exploring. The challenge isn\u2019t finding applications for blockchain AI, we\u2019re already seeing numerous compelling use cases. The challenge is ensuring these applications are sustainable, secure, and truly valuable to users.\nWe\u2019re focusing on three key areas:\nDeveloper Education: Making verifiable AI concepts accessible to mainstream blockchain developers\nTooling Refinement: Improving the developer experience for building AI-integrated applications\nCommunity Building: Attracting more developers to explore the possibilities of verifiable AI\nThe Skills Evolution\nThe skill requirements for blockchain developers are rapidly evolving. Traditional blockchain development skills remain important, but developers now need to understand:\nAI model integration and management\nVerifiable computing concepts\nTEE-based security architecture\nCross-chain AI orchestration\nPrivacy-preserving AI techniques\nConclusion\nThe transformation I\u2019ve witnessed at Metis represents more than just technological advancement\u2014it\u2019s a fundamental shift in how we approach decentralized application development. AI isn\u2019t replacing developers; it\u2019s amplifying their capabilities and enabling them to solve problems that were previously intractable.\nAs we continue to refine our tools and expand our developer community, I\u2019m convinced that the future of blockchain development will be defined by those who can effectively combine AI\u2019s capabilities with blockchain\u2019s decentralization principles. The projects emerging from our hackathon and developer community demonstrate that this future is already here, and it\u2019s more exciting than we ever imagined.\nThe key to success in this new paradigm is understanding that AI and blockchain aren\u2019t competing technologies, they\u2019re complementary forces that, when properly integrated, can create applications that are both intelligent and truly decentralized. At Metis, we\u2019re committed to leading this transformation and providing developers with the tools they need to build the next generation of Web3 applications.",
        "comments_details": [
            {
                "author": "pavel",
                "comment": "As the Developer Experience Lead at Metis, a Layer 2 blockchain project focused on AI integration, I\u2019ve witnessed firsthand how artificial intelligence is fundamentally transforming how developers approach blockchain development. We\u2019re not just seeing incremental improvements, we\u2019re experiencing a complete paradigm shift in how developers learn, build, and deploy applications in the Web3 space.\nAI as First-Class Infrastructure\nAt Metis, we\u2019ve made a strategic decision to treat AI as first-class infrastructure rather than an afterthought. This means AI isn\u2019t just a tool we use occasionally; it\u2019s deeply integrated into every aspect of our developer experience. From our documentation system that has access to all of Metis\u2019 data sources to Alith, our core AI agent framework, we\u2019ve built AI into the foundation of how developers interact with our ecosystem.\nThe impact has been transformative. Just as developers have long relied on boilerplate code to accelerate project development, AI has become the new boilerplate, a foundation where developers can rapidly add features and functions to solve specific problems. This shift has fundamentally changed the development workflow from lengthy setup processes to rapid iteration and deployment.\nThe Alith Framework: Redefining AI Agent Development\nOur flagship contribution to this transformation is Alith, Metis\u2019 main AI agent framework. Alith represents what we believe is the future of blockchain-AI integration, offering several key capabilities:\nLazAI Gateway provides comprehensive wallet management, transaction handling, and access to core privacy features like iDAO, DAT, and verified computing. This eliminates the complexity traditionally associated with blockchain wallet integration.\nMultiple Model Support ensures developers aren\u2019t locked into a single AI provider. We support both small models and large language models from providers like Llama, Grok, OpenAI, and Anthropic, giving developers flexibility in choosing the right tool for their specific use case.\nHigh Extensibility allows developers to customize everything from internal prompts to low-level API access. They can define roles, goals, tools, actions, and behaviors while maintaining clean abstraction layers.\nWorkflow Support enables complex orchestration patterns beyond simple sequential processes. Developers can implement hierarchical processes, conditional branching, and parallel execution\u2014crucial for sophisticated AI applications.\nCross-Language Support with SDKs for Rust, Python, and Node.js makes Alith accessible to developers regardless of their preferred programming language.\nHigh-Performance AI Training and Inference leverages Rust\u2019s performance advantages along with graph optimization, model compression, and JIT/AOT compilation with GPU coprocessors.\nWeb3 Friendly and Secure architecture provides out-of-the-box Web3 plugins that allow developers to securely integrate blockchain capabilities into TEE-based AI agent frameworks.\nVerifiable AI: The Missing Piece in Decentralized Development\nOne of the most critical innovations we\u2019re pioneering is verifiable AI. This concept addresses a fundamental problem in decentralized applications: when we give access to centralized AI providers to manage our actions, those providers can introduce actions that might appear to be AI outputs but are actually provider-introduced manipulations.\nVerifiable AI is the cornerstone of operations in the decentralized space. We achieve this through two primary mechanisms:\nOn-chain Inferencing Records that create an immutable trail of AI decision-making processes\nTrusted Execution Environments (TEEs) that ensure AI runs in secure, isolated environments\nFuture Zero-Knowledge Proofs that will provide cryptographic verification of AI operations\nWhen considering the trade-offs between speed, cost, and complexity, the minimal additional cost of verifiable AI becomes insignificant compared to the security and risk implications of using unverifiable AI in financial and governance applications.\nReal-World Applications: From Hackathons to Production\nThe practical applications emerging from our developer community demonstrate the power of this AI-first approach. In our current hackathon, we\u2019re seeing innovative projects that showcase the potential of verifiable AI in blockchain applications:\nRugRadar: AI-based risk assessment for Web3 projects, providing due diligence and risk scoring\nPortfolioAI: Making DeFi portfolio management more engaging through intelligent automation\nEagleDAC: AI-powered DAC generator and real-time smart contract auditor for building decentralized autonomous companies\nHyperChain Insights: AI-powered real-time transaction analytics for blockchain monitoring\nSentinel: AI tools for analyzing smart contracts and identifying vulnerabilities\nAlithGuard: Real-time AI-powered transaction and dApp sentinel for protecting users from scams and malicious contracts\nThese projects demonstrate that developers are thinking beyond simple automation to create sophisticated security, analytics, and governance tools that leverage AI\u2019s capabilities while maintaining blockchain\u2019s decentralization principles.\nThe Developer Experience Transformation\nFrom my perspective leading developer experience, the most significant change is how AI has accelerated the learning curve for new blockchain developers. Our AI-integrated documentation system allows developers with limited knowledge of the Metis ecosystem to quickly understand available projects and how our blockchain differentiates from others.\nThe common workflow patterns we see include:\nSocial bots for Slack, Telegram, and Twitter that work with Retrieval-Augmented Generation (RAG)\nOn-chain interaction bots that can execute complex blockchain operations\nReal-time analytics systems that process and interpret blockchain data\nSecurity monitoring tools that identify and prevent malicious activities\nChallenges and Education\nThe biggest challenge we face isn\u2019t technical, it\u2019s educational. Most developers have adapted to using AI in their work, from writing proposals to actual code development. However, the concept of verifiable AI requires a fundamental shift in how developers architect their applications.\nThe main pain point we address through our weekly workshops is simply \u201chow to build using Alith.\u201d We\u2019re continuously developing more examples and educational materials to outline proper usage patterns. The key is helping developers understand not just the technical implementation, but the broader purpose of building applications with verifiable AI.\nThe Competitive Landscape\nAs the only Layer 2 blockchain taking this approach of full AI integration at the blockchain level, we\u2019re pioneering a new category of Web3 infrastructure. While other projects are experimenting with AI tools, we\u2019re building a comprehensive suite of tools that make blockchain-AI integration fundamentally easier.\nOur approach differs from traditional AI integrations in several ways:\nAI is built into the protocol level, not just application level\nVerifiability is a core requirement, not an afterthought\nCross-language support ensures broad developer adoption\nTEE-based security provides enterprise-grade protection\nLooking Forward: Sustainability and Refinement\nWithin the next 6-12 months, I expect to see significant refinement of the use cases we\u2019re currently exploring. The challenge isn\u2019t finding applications for blockchain AI, we\u2019re already seeing numerous compelling use cases. The challenge is ensuring these applications are sustainable, secure, and truly valuable to users.\nWe\u2019re focusing on three key areas:\nDeveloper Education: Making verifiable AI concepts accessible to mainstream blockchain developers\nTooling Refinement: Improving the developer experience for building AI-integrated applications\nCommunity Building: Attracting more developers to explore the possibilities of verifiable AI\nThe Skills Evolution\nThe skill requirements for blockchain developers are rapidly evolving. Traditional blockchain development skills remain important, but developers now need to understand:\nAI model integration and management\nVerifiable computing concepts\nTEE-based security architecture\nCross-chain AI orchestration\nPrivacy-preserving AI techniques\nConclusion\nThe transformation I\u2019ve witnessed at Metis represents more than just technological advancement\u2014it\u2019s a fundamental shift in how we approach decentralized application development. AI isn\u2019t replacing developers; it\u2019s amplifying their capabilities and enabling them to solve problems that were previously intractable.\nAs we continue to refine our tools and expand our developer community, I\u2019m convinced that the future of blockchain development will be defined by those who can effectively combine AI\u2019s capabilities with blockchain\u2019s decentralization principles. The projects emerging from our hackathon and developer community demonstrate that this future is already here, and it\u2019s more exciting than we ever imagined.\nThe key to success in this new paradigm is understanding that AI and blockchain aren\u2019t competing technologies, they\u2019re complementary forces that, when properly integrated, can create applications that are both intelligent and truly decentralized. At Metis, we\u2019re committed to leading this transformation and providing developers with the tools they need to build the next generation of Web3 applications."
            }
        ]
    }
]